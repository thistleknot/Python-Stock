{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3139d-1ac0-4486-80f0-e54d08259ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "ee726465-0b88-46f7-877f-5aee84616ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import shap\n",
    "import numpy as np\n",
    "import ZCA as zca\n",
    "import statsmodels.api as sm\n",
    "import matplotlib as plt\n",
    "\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython\n",
    "\n",
    "import os\n",
    "os.environ['R_HOME'] = '/mnt/distvol/R/4.0.5/lib64/R/'\n",
    "import rpy2.robjects as R\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import arange\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import seaborn as sns\n",
    "from ModelDiagnostics import Plot\n",
    "from sklearn.cluster import DBSCAN\n",
    "from clustergram import Clustergram\n",
    "import urbangrammar_graphics as ugg\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from scipy.stats import f\n",
    "\n",
    "import dtale\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf8827-4f34-413b-9a4d-bd6565e33372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#power = PowerTransformer(method='box-cox')\n",
    "\n",
    "def testNormal (x):    \n",
    "    \n",
    "    k2, p = stats.normaltest(x)\n",
    "    alpha = .001\n",
    "    #print(\"p = {:g}\".format(p))    \n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        #print(p)\n",
    "        #print(alpha)\n",
    "        print(\"The null hypothesis can be rejected\")\n",
    "        xt, _ = stats.yeojohnson(x)\n",
    "        #xt, _ = stats.boxcox(x)        \n",
    "        print(_)\n",
    "        xt = pd.DataFrame(xt)\n",
    "        \n",
    "        return _, pd.DataFrame(xt).set_index(x.index)\n",
    "    else:\n",
    "        print(\"The null hypothesis cannot be rejected\")    \n",
    "        return 1, pd.DataFrame(x)\n",
    "\n",
    "def inverse_boxcox (data, lambdas):\n",
    "    return inv_boxcox(data, lambdas.values)\n",
    "    \n",
    "def transform_boxcox_l(data, l_):\n",
    "    transformed = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        #print(i)\n",
    "        if l_.iloc[i].values == 1:\n",
    "            inner_scale = data.iloc[:,i]            \n",
    "        else:\n",
    "            inner_scale = pd.DataFrame(stats.boxcox((data.iloc[:,i]), lmbda=l_.iloc[i].values))\n",
    "            \n",
    "        inner_scale.index = data.index\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed\n",
    "\n",
    "def transform_boxcox (data):\n",
    "    transformed = pd.DataFrame()\n",
    "    transformed_lambdas = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        l, inner_scale = testNormal(data.iloc[:,i])\n",
    "        inner_scale.set_index(data.index)\n",
    "\n",
    "        transformed_lambdas = pd.concat([transformed_lambdas,pd.DataFrame(pd.Series(l))],axis=0)\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed, transformed_lambdas\n",
    "\n",
    "def revert_boxcox (data, lambdas):\n",
    "    reverted = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):        \n",
    "        if lambdas.iloc[i].values == 1 :\n",
    "            revert = data.iloc[:,i]\n",
    "        else:\n",
    "            revert = pd.DataFrame(inv_boxcox(data.iloc[:,i].values, lambdas.iloc[i].values))            \n",
    "        revert.index = data.index\n",
    "        reverted = pd.concat([reverted,revert],axis=1)\n",
    "        \n",
    "    reverted.columns = data.columns\n",
    "    return reverted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "id": "2ec9d431-186e-4171-8a8a-378ab2ab2e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_deltas(data):\n",
    "\n",
    "    R.r('''\n",
    "               f <- function(values) {\n",
    "                        #system(\"which openssl\")\n",
    "\n",
    "                        library(snpEnrichment)\n",
    "                        library(arfima)\n",
    "                        library(parallel)\n",
    "                        library(forecast)                    \n",
    "\n",
    "                        dset <- lapply(1:ncol(values),function(x)\n",
    "                        {\n",
    "                            column = values[,x]\n",
    "\n",
    "\n",
    "                            #tryCatch(invisible(capture.output(suppressMessages(suppressWarnings(\n",
    "                            #{\n",
    "                              varvefd = arfima(column)\n",
    "                              d = summary(varvefd)$coef[[1]][1]\n",
    "                              \n",
    "                              r = residuals(varvefd, reg = TRUE)\n",
    "                              #print(r)\n",
    "                              return(d)\n",
    "                            #}\n",
    "                           #)\n",
    "                           #))),\n",
    "                            #error=function(e)\n",
    "                              #{\n",
    "                                #d = 1\n",
    "                                #return(d)\n",
    "                              #})\n",
    "\n",
    "                        })    \n",
    "\n",
    "                        unlist(dset)\n",
    "\n",
    "                }\n",
    "                ''')\n",
    "\n",
    "    r_f = R.globalenv['f']\n",
    "    d=R.conversion.rpy2py((r_f(R.conversion.py2rpy(data.dropna()))))\n",
    "    return(d)\n",
    "\n",
    "def get_residuals(data):\n",
    "\n",
    "    R.r('''\n",
    "               f <- function(values) {\n",
    "                        #system(\"which openssl\")\n",
    "\n",
    "                        library(snpEnrichment)\n",
    "                        library(arfima)\n",
    "                        library(parallel)\n",
    "                        library(forecast)                    \n",
    "\n",
    "                        dset <- lapply(1:ncol(values),function(x)\n",
    "                        {\n",
    "                            column = values[,x]\n",
    "\n",
    "\n",
    "                            #tryCatch(invisible(capture.output(suppressMessages(suppressWarnings(\n",
    "                            #{\n",
    "                              varvefd = arfima(column)\n",
    "                              d = summary(varvefd)$coef[[1]][1]\n",
    "                              \n",
    "                              #return(d)\n",
    "                              r = residuals(varvefd, reg = TRUE)\n",
    "                              #print(r)\n",
    "                              return(r)\n",
    "                            #}\n",
    "                           #)\n",
    "                           #))),\n",
    "                            #error=function(e)\n",
    "                              #{\n",
    "                                #d = 1\n",
    "                                #return(d)\n",
    "                              #})\n",
    "\n",
    "                        })    \n",
    "\n",
    "                        unlist(dset)\n",
    "\n",
    "                }\n",
    "                ''')\n",
    "\n",
    "    r_f = R.globalenv['f']\n",
    "    d=R.conversion.rpy2py((r_f(R.conversion.py2rpy(data.dropna()))))\n",
    "    return(d)\n",
    "\n",
    "def arfima_predict(data_, ahead):\n",
    "\n",
    "    R.r('''\n",
    "               f_ <- function(values, ahead) {\n",
    "                        #system(\"which openssl\")\n",
    "                        print(nrow(values))\n",
    "\n",
    "                        library(snpEnrichment)\n",
    "                        library(arfima)\n",
    "                        library(parallel)\n",
    "                        library(forecast)                    \n",
    "\n",
    "                        dset <- lapply(1:ncol(values),function(x)\n",
    "                        {\n",
    "                            column = values[,x,drop=FALSE]\n",
    "\n",
    "\n",
    "                            #tryCatch(invisible(capture.output(suppressMessages(suppressWarnings(\n",
    "                            #{\n",
    "                              varvefd = arfima(column)\n",
    "                              d = summary(varvefd)$coef[[1]][1]\n",
    "                              \n",
    "                              #return(d)\n",
    "                              r = residuals(varvefd, reg = TRUE)\n",
    "                              #print(r)\n",
    "                              #return(r)\n",
    "                              \n",
    "                              pred <- predict(varvefd, ahead)\n",
    "                              #print(ahead)\n",
    "                              #print(pred)\n",
    "                              #print(pred)\n",
    "                              return(as.data.frame(pred)[,1,drop=TRUE])\n",
    "                            #}\n",
    "                           #)\n",
    "                           #))),\n",
    "                            #error=function(e)\n",
    "                              #{\n",
    "                                #d = 1\n",
    "                                #return(d)\n",
    "                              #})\n",
    "\n",
    "                        })    \n",
    "\n",
    "                        unlist(dset)\n",
    "\n",
    "                }\n",
    "                ''')\n",
    "\n",
    "    r_f_ = R.globalenv['f_']\n",
    "    d_=R.conversion.rpy2py((r_f_(R.conversion.py2rpy(data_),ahead)))\n",
    "    return(d_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "id": "e43598e7-17dd-4ced-b588-e8870d3df933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "def arima_adjust(y_train, y_test, train_predicted, test_predicted):\n",
    "    \n",
    "    #arima residual adjusted\n",
    "    \n",
    "    window = 15\n",
    "    train_resid = y_train - pd.DataFrame(train_predicted).set_index(y_train.index)\n",
    "\n",
    "    model = AutoReg(train_resid.values, lags=15)\n",
    "    model_fit = model.fit()\n",
    "    coef = model_fit.params\n",
    "    # walk forward over time steps in test\n",
    "    history = train_resid[len(train_resid)-window:]\n",
    "    history = [history.loc[history.index[i]] for i in range(len(history))]\n",
    "    predictions = list()\n",
    "    for t in range(len(y_test)):\n",
    "        # persistence\n",
    "        yhat = pd.DataFrame(test_predicted).set_index(y_test.index).loc[y_test.index[t]]    \n",
    "        error = y_test.loc[y_test.index[t]] - yhat\n",
    "        # predict error\n",
    "        length = len(history)\n",
    "        lag = [history[i] for i in range(length-window,length)]\n",
    "        pred_error = coef[0]\n",
    "        for d in range(window):\n",
    "            pred_error += coef[d+1] * lag[window-d-1]\n",
    "        # correct the prediction\n",
    "        yhat = yhat + pred_error\n",
    "        #print(yhat)\n",
    "        predictions.append(np.round(yhat,0).astype(int))\n",
    "        history.append(error)\n",
    "        #print('predicted=%f, expected=%f' % ((np.round(yhat,0)), y_test.loc[y_test.index[t]]))\n",
    "\n",
    "    print(metrics.classification_report(y_test,predictions))\n",
    "    print(metrics.confusion_matrix(y_test,predictions))\n",
    "    \n",
    "    return(pd.DataFrame(predictions).set_index(y_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af9904-08a5-4d5f-90f7-1b4c5d4652df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9bb80-d23a-4a60-964b-41dad96d6411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1448,
   "id": "1235085d-f98a-4b9d-a118-29a6f154af93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom statsmodels.tsa.ar_model import AutoReg\\n\\nresiduals_train = y_train - pd.DataFrame(clf.predict(X_train)).set_index(y_train.index)\\n\\ny_adjusted = predicteds.copy()\\n\\nfor i in range(0,len(y_test)):    \\n    \\n    if i == 0:\\n        forecast_r = arfima_predict(residuals_train.values)\\n        \\n    else:\\n        residuals_test = y_test.loc[y_test.index[0:i]] - pd.DataFrame(predicteds).set_index(y_test.index).loc[y_test.index[0:i]]\\n        \\n        residuals = pd.concat([residuals_train,residuals_test],axis=0)\\n        \\n        forecast_r = arfima_predict(residuals.values)\\n        \\n    print(forecast_r)\\n    adjusted = pd.DataFrame(y_adjusted).set_index(y_test.index).loc[y_test.index[i]]\\n    #print(adjusted)\\n    \\n    \\n    \\n    #y_test.loc[0:i]-pd.DataFrame(predicteds).set_index(y_test.loc[0:i].index)\\n    #ar_s_model = AutoReg(residuals, lags=15)\\n    #model_fit = model.fit()\\n    #plt.plot(np.round(arfima_predict(residuals.values, len(y_test)),0))\\n\\n    #improved_forecast = forecast + estimated error\\n\\n    #print('Coef=%s' % (model_fit.params))\\n\""
      ]
     },
     "execution_count": 1448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#arfima residual adjusted\n",
    "def arfima_adjust(y_train, y_test, train_predicted, test_predicted):\n",
    "    \n",
    "    #train_resid = get_deltas(y_train)\n",
    "    train_resid = y_train - pd.DataFrame(train_predicted).set_index(y_train.index)\n",
    "\n",
    "    y_adjusted = predicted.copy()\n",
    "\n",
    "    forecast_r = arfima_predict(train_resid.values, len(y_test))  \n",
    "    print(len(forecast_r))\n",
    "    \n",
    "    # walk forward over time steps in test\n",
    "    #history = train_resid[len(train_resid):-1]\n",
    "    #history = [history.loc[history.index[i]] for i in range(len(history))]\n",
    "    predictions = list()\n",
    "    for t in range(len(y_test)):\n",
    "        # persistence\n",
    "        yhat = pd.DataFrame(test_predicted).set_index(y_test.index).loc[y_test.index[t]]\n",
    "        #error = y_test.loc[y_test.index[t]] - yhat\n",
    "        # predict error\n",
    "        #length = len(history)\n",
    "        #lag = [history[i] for i in range(length-window,length)]\n",
    "        #pred_error = coef[0]\n",
    "        #for d in range(window):\n",
    "            #pred_error += coef[d+1] * lag[window-d-1]\n",
    "        # correct the prediction\n",
    "        #print(yhat)\n",
    "        #print(pd.DataFrame(forecast_r).set_index(y_test.index).loc[y_test.index[t]] )\n",
    "        yhat = yhat + pd.DataFrame(forecast_r).set_index(y_test.index).loc[y_test.index[t]] \n",
    "        #print(yhat)\n",
    "        predictions.append(np.round(yhat,0).astype(int))\n",
    "        #history.append(error)\n",
    "        #print('predicted=%f, expected=%f' % ((np.round(yhat,0)), y_test.loc[y_test.index[t]]))\n",
    "\n",
    "    print(metrics.classification_report(y_test,predictions))\n",
    "    print(metrics.confusion_matrix(y_test,predictions))\n",
    "    \n",
    "    return(pd.DataFrame(predictions).set_index(y_test.index))\n",
    "\n",
    "'''\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "residuals_train = y_train - pd.DataFrame(clf.predict(X_train)).set_index(y_train.index)\n",
    "\n",
    "y_adjusted = predicteds.copy()\n",
    "\n",
    "for i in range(0,len(y_test)):    \n",
    "    \n",
    "    if i == 0:\n",
    "        forecast_r = arfima_predict(residuals_train.values)\n",
    "        \n",
    "    else:\n",
    "        residuals_test = y_test.loc[y_test.index[0:i]] - pd.DataFrame(predicteds).set_index(y_test.index).loc[y_test.index[0:i]]\n",
    "        \n",
    "        residuals = pd.concat([residuals_train,residuals_test],axis=0)\n",
    "        \n",
    "        forecast_r = arfima_predict(residuals.values)\n",
    "        \n",
    "    print(forecast_r)\n",
    "    adjusted = pd.DataFrame(y_adjusted).set_index(y_test.index).loc[y_test.index[i]]\n",
    "    #print(adjusted)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #y_test.loc[0:i]-pd.DataFrame(predicteds).set_index(y_test.loc[0:i].index)\n",
    "    #ar_s_model = AutoReg(residuals, lags=15)\n",
    "    #model_fit = model.fit()\n",
    "    #plt.plot(np.round(arfima_predict(residuals.values, len(y_test)),0))\n",
    "\n",
    "    #improved_forecast = forecast + estimated error\n",
    "\n",
    "    #print('Coef=%s' % (model_fit.params))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a133da3-b0fd-4741-a0aa-db8cc75f878d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_weights(d, num_k):\n",
    "    r\"\"\"Calculate weights ($w$) for each lag ($k$) through\n",
    "    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$.\n",
    "    \n",
    "    Args:\n",
    "        d (int): differencing value.\n",
    "        num_k (int): number of lags (typically length of timeseries) to calculate w.\n",
    "    \"\"\"\n",
    "    w_k = np.array([1])\n",
    "    \n",
    "    for k in range(1, num_k):\n",
    "        w_k = np.append(w_k, -w_k[-1] * ((d - k + 1)) / k)\n",
    "        \n",
    "    w_k = w_k.reshape(-1, 1) \n",
    "    \n",
    "    return w_k\n",
    "\n",
    "def get_weights_floored(d, num_k, floor=1e-3):\n",
    "    r\"\"\"Calculate weights ($w$) for each lag ($k$) through\n",
    "    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$ provided weight above a minimum value\n",
    "    (floor) for the weights to prevent computation of weights for the entire\n",
    "    time series.\n",
    "    \n",
    "    Args:\n",
    "        d (int): differencing value.\n",
    "        num_k (int): number of lags (typically length of timeseries) to calculate w.\n",
    "        floor (float): minimum value for the weights for computational efficiency.\n",
    "    \"\"\"\n",
    "    w_k = np.array([1])\n",
    "    k = 1\n",
    "    \n",
    "    while k < num_k:\n",
    "        w_k_latest = -w_k[-1] * ((d - k + 1)) / k\n",
    "        if abs(w_k_latest) <= floor:\n",
    "            break\n",
    "\n",
    "        w_k = np.append(w_k, w_k_latest)\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    w_k = w_k.reshape(-1, 1) \n",
    "    \n",
    "    return w_k\n",
    "\n",
    "def frac_diff(df, d, floor=1e-3):\n",
    "    r\"\"\"Fractionally difference time series via CPU.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe of raw time series values.\n",
    "        d (float): differencing value from 0 to 1 where > 1 has no FD.\n",
    "        floor (float): minimum value of weights, ignoring anything smaller.\n",
    "    \"\"\"\n",
    "    # Get weights window\n",
    "    weights = get_weights_floored(d=d, num_k=len(df), floor=floor)\n",
    "    weights_window_size = len(weights)\n",
    "    \n",
    "    # Reverse weights\n",
    "    weights = weights[::-1]\n",
    "    \n",
    "    # Blank fractionally differenced series to be filled\n",
    "    df_fd = []\n",
    "\n",
    "    # Slide window of time series, to calculated fractionally differenced values\n",
    "    # per window\n",
    "    for idx in range(weights_window_size, df.shape[0]):\n",
    "        # Dot product of weights and original values\n",
    "        # to get fractionally differenced values\n",
    "        date_idx = df.index[idx]\n",
    "        df_fd.append(np.dot(weights.T, df.iloc[idx - weights_window_size:idx]).item())\n",
    "    \n",
    "    # Return FD values and weights\n",
    "    df_fd = pd.DataFrame(df_fd)\n",
    "    \n",
    "    return df_fd, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e53d3-2864-4d69-a45a-ce48217e4fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "d949ef06-ae96-4828-b02b-21a92a11b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxl = 5\n",
    "train_size = .7\n",
    "t_size = 1-train_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "62e6233b-8abf-4f78-83f5-a3e7a2e26fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data = pd.read_csv('/mnt/distvol/combined_set.csv')\n",
    "all_data.index = all_data.iloc[:,0]\n",
    "all_data = all_data.iloc[:,1:]\n",
    "\n",
    "filter_ = all_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "id": "ca4d80c7-99c2-4070-b6f4-1b19d28520f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003-06-29\n",
      "2005-04-17\n",
      "2007-02-04\n",
      "2008-11-23\n",
      "2010-09-12\n",
      "2012-07-01\n",
      "2014-04-20\n",
      "2016-02-07\n",
      "2017-11-26\n",
      "2019-09-15\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "8b2b9636-4857-4aaa-b13d-e2077e957c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose Y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7582ebc697274d71931f4a855a8417f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Y', options=('DGS10', 'DTB3', 'DGS3MO', 'MORTGAGE30US', 'DFII10', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f3(Y):\n",
    "    \n",
    "    #Y = x\n",
    "    #output_slider_variable.value\n",
    "    internalFilter = filter_.copy()\n",
    "    internalFilter.remove(Y)\n",
    "    all_data_ = pd.concat([all_data[Y],all_data[internalFilter]], axis=1)    \n",
    "    #print(all_data_.describe())\n",
    "    display(all_data_.describe())\n",
    "    #x_ticks = all_data_.index[np.arange(0, len(all_data.index), int(len(internalFilter)/5))]\n",
    "    x_ticks  = []\n",
    "    for index, element in enumerate(all_data_.index):\n",
    "        if index % int(np.round(len(all_data_.index)/10)) == 0:\n",
    "            x_ticks.append(element)\n",
    "    plt.plot(all_data_[Y])\n",
    "    plt.xticks(x_ticks, rotation = 45)\n",
    "    plt.show()        \n",
    "    plt.hist(all_data_[Y], bins='auto')\n",
    "    plt.show()\n",
    "    diff = pd.DataFrame((all_data_[Y].pct_change())).dropna()\n",
    "    plt.hist(diff, bins='auto')\n",
    "    plt.show()\n",
    "    return(all_data_)\n",
    "    \n",
    "out = interactive(f3, Y=filter_)\n",
    "\n",
    "#output_slider_variable.observe(f4, 'value')\n",
    "\n",
    "print(\"choose Y\")\n",
    "display(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2cc18-8baf-4f8a-bcd2-9562b62237fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(out.result, test_size=tsize, shuffle=False)\n",
    "\n",
    "d = get_deltas(train)\n",
    "#differenced = pd.DataFrame(np.transpose(d.reshape(len(out.result.columns),len(out.result))))\n",
    "#differenced = pd.DataFrame(np.transpose(d.reshape(len(out.result.columns),len(out.result))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1487,
   "id": "2d87fad8-9555-4fe1-a82f-18243715d650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748"
      ]
     },
     "execution_count": 1487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1489,
   "id": "2ed2b0f5-1e32-4e6a-8e54-ea4dbe225df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1469,
   "id": "33e1c5c4-9718-4cbb-a820-9fd94e3fc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import concurrent.futures\n",
    "from concurrent.futures import wait, ALL_COMPLETED\n",
    "from fracdiff import fdiff\n",
    "\n",
    "cores = int(len(os.sched_getaffinity(0)))\n",
    "\n",
    "def getDifferenced(i, data):\n",
    "    #v = d[[i]]\n",
    "    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i], d=d[[i]], floor=5e-5)\n",
    "    #print(i)\n",
    "    a = np.array(data.iloc[:, i])\n",
    "    \n",
    "    return fdiff(a, n=d[i], axis=0)\n",
    "    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i]), d=v, floor=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1470,
   "id": "2070d863-3329-4f28-8b4e-a74b5480cc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndifferenced = pd.DataFrame()\\nfor f in range(0,len(futures01)):\\n    value = pd.DataFrame(futures01[f].result())\\n    differenced = pd.concat([differenced,value],axis=1)\\n    \\ndifferenced.columns = train.columns\\ndifferenced.index = train.index\\n'"
      ]
     },
     "execution_count": 1470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "differenced = pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(d)):\n",
    "    value = pd.DataFrame(getDifferenced(i, out.result))\n",
    "    #print(value)\n",
    "    differenced = pd.concat([differenced,value],axis=1)\n",
    "    \n",
    "differenced.columns = out.result.columns\n",
    "differenced.index = out.result.index    \n",
    "\n",
    "'''\n",
    "\n",
    "differenced_train = pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(d)):\n",
    "    value = pd.DataFrame(getDifferenced(i, train))\n",
    "    #print(value)\n",
    "    differenced_train = pd.concat([differenced_train,value],axis=1)\n",
    "    \n",
    "differenced_train.columns = train.columns\n",
    "differenced_train.index = train.index    \n",
    "\n",
    "differenced_test = pd.DataFrame()\n",
    "for i in range(0,len(d)):\n",
    "    value = pd.DataFrame(getDifferenced(i, test))\n",
    "    differenced_test = pd.concat([differenced_test,value],axis=1)\n",
    "\n",
    "differenced_test.columns = test.columns\n",
    "differenced_test.index = test.index\n",
    "\n",
    "differenced = pd.DataFrame()\n",
    "differenced = pd.concat([differenced_train,differenced_test],axis=0)\n",
    "'''\n",
    "differenced = pd.concat([out.result.iloc[:,0],differenced.iloc[:,1:]],axis=1)\n",
    "\n",
    "#pool01 = concurrent.futures.ProcessPoolExecutor(cores)\n",
    "\n",
    "#futures01 = [pool01.submit(getDifferenced, args) for args in (range(0,len(d)))]\n",
    "\n",
    "#wait(futures01, timeout=None, return_when=ALL_COMPLETED)\n",
    "#'''\n",
    "\n",
    "'''\n",
    "differenced = pd.DataFrame()\n",
    "for f in range(0,len(futures01)):\n",
    "    value = pd.DataFrame(futures01[f].result())\n",
    "    differenced = pd.concat([differenced,value],axis=1)\n",
    "    \n",
    "differenced.columns = train.columns\n",
    "differenced.index = train.index\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2d32f-b9a1-4196-9d43-1359b65f1c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "f492f94d-cba1-455a-b1d4-157a346f6d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://slurmw01:40000'"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ = dtale.show(out.result)\n",
    "d_.open_browser()\n",
    "d_._url  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "4273fdd8-d805-4928-b7dd-bda427cf51fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport concurrent.futures\\nfrom concurrent.futures import wait, ALL_COMPLETED\\nfrom fracdiff import fdiff\\n\\ncores = int(len(os.sched_getaffinity(0)))\\n\\ndef getDifferenced(i):\\n    #v = d[[i]]\\n    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i], d=d[[i]], floor=5e-5)\\n    \\n    a = np.array(out.result.iloc[:, i])\\n    \\n    return fdiff(a, n=d[i], axis=0)\\n    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i]), d=v, floor=5e-5)\\n\\npool01 = concurrent.futures.ProcessPoolExecutor(cores)\\n\\nfutures01 = [pool01.submit(getDifferenced, args) for args in range(0,len(d))]\\n\\nwait(futures01, timeout=None, return_when=ALL_COMPLETED)\\n'"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfa758-3cf3-4410-bfc6-85105b37a19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e2820-110a-4786-9035-98543c301b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for f in range(0,len(futures01)):\n",
    "        #print(f)\n",
    "        #print(len(futures01[f].result()))\n",
    "        plt.hist(Differenced_Set.iloc[:,f], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "        Differenced_Set.iloc[:,1].plot()\n",
    "        plt.show()\n",
    "        plt.hist(all_data.iloc[:,f], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "        all_data.iloc[:,1].plot()\n",
    "        plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce862ad-f7fc-4f0b-b376-c29d32d01281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b6b7d-d0c5-4077-b822-d79e626067b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "3d49f670-bbdb-42a8-b9e3-e75e1b60e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = out.result.corr()\n",
    "#.abs()\n",
    "s = c.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0016982-6857-424e-868a-da44b16ff66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece4be0-8244-434e-b66e-9c1f39a7c2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1494,
   "id": "4d4a92b7-e5b3-49ed-b31f-5959f90e8fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "def critical_r(n, alpha = .05 ):\n",
    "    df = n - 2\n",
    "    critical_t = scipy.stats.t.isf(alpha / 2, df)\n",
    "    critical_r = np.sqrt( (critical.t^2) / ( (critical.t^2) + df ) )\n",
    "    return(critical_r)\n",
    "\n",
    "def xcorr(x, y, maxlags=10):\n",
    "    Nx = len(x)\n",
    "    if Nx != len(y):\n",
    "        raise ValueError('x and y must be equal length')\n",
    "\n",
    "    c = np.correlate(x, y, mode=2)\n",
    "\n",
    "    if maxlags is None:\n",
    "        maxlags = Nx - 1\n",
    "\n",
    "    if maxlags >= Nx or maxlags < 1:\n",
    "        raise ValueError('maxlags must be None or strictly positive < %d' % Nx)\n",
    "\n",
    "    c = c[Nx - 1 - maxlags:Nx + maxlags]\n",
    "\n",
    "    return c\n",
    "\n",
    "def getLagged_Set(set_, og):\n",
    "\n",
    "    train, test = train_test_split(set_, test_size=tsize, shuffle=False)\n",
    "    \n",
    "    residuals = pd.DataFrame(get_residuals(train).reshape(len(train),len(train.columns)))\n",
    "\n",
    "    residuals.columns = train.columns\n",
    "    residuals.index = train.index\n",
    "\n",
    "    #print(len(train))\n",
    "    #set_.index = all_data.index\n",
    "    \n",
    "    Lagged_Differenced_Set = pd.DataFrame()\n",
    "    Lagged_Set = pd.DataFrame()\n",
    "    lags = []\n",
    "    lagcorrs = []\n",
    "    ogcorrs = []\n",
    "\n",
    "    for f in range(1,len(train.columns)):\n",
    "        #print(f)\n",
    "        #print(len(futures01[f].result()))\n",
    "\n",
    "        data_1 = residuals.iloc[:,0]\n",
    "        data_2 = residuals.iloc[:,f]\n",
    "\n",
    "        ogc = np.array(pd.concat([data_1 - np.mean(data_1),data_2 - np.mean(data_2)],axis=1).corr())[1,0]    \n",
    "        ogcorrs.append(ogc)\n",
    "\n",
    "        #corr = xcorr(data_1 - np.mean(data_1), data_2 - np.mean(data_2),maxlags=5)\n",
    "\n",
    "        set1 = data_1 - np.mean(data_1)\n",
    "        set2 = data_2 - np.mean(data_2)\n",
    "\n",
    "        corrs_ = []\n",
    "        for i in range(0,maxl):\n",
    "            c = np.array((pd.concat([set1,set2.shift(i)],axis=1).dropna()).corr())[0,1]\n",
    "            corrs_.append(c)\n",
    "\n",
    "        #corr = np.correlate(data_1 - np.mean(data_1), data_2 - np.mean(data_2),mode='full')\n",
    "        #plt.plot(corr)\n",
    "        #plt.show()\n",
    "\n",
    "        #lag = corr.argmax() - (len(data_1) - 1)\n",
    "        lag = abs(pd.Series(corrs_)).idxmax()\n",
    "\n",
    "        #print(corr)\n",
    "        lagc = np.array(pd.concat([data_1 - np.mean(data_1),(data_2 - np.mean(data_2)).shift(lag)],axis=1).corr())[1,0]\n",
    "\n",
    "        #print(lag)\n",
    "\n",
    "        #print(ogc)\n",
    "        #print(lagc)\n",
    "\n",
    "        #print(lag)\n",
    "        #plt.plot(data_1, 'r*')\n",
    "        #plt.plot(data_2, 'b*')\n",
    "\n",
    "        lag_merged = pd.concat([data_1 - np.mean(data_1),(data_2 - np.mean(data_2)).shift(lag)],axis=1)\n",
    "\n",
    "        #x_ticks = all_data.index[np.arange(0, len(all_data.index), 5)]\n",
    "        #plt.xticks(x_ticks, rotation = 45)    \n",
    "        #plt.show()\n",
    "\n",
    "        #plt.scatter(data_2.shift(lag),data_1)\n",
    "        #plt.show()\n",
    "\n",
    "        #plot_acf(data_2.shift(lag))\n",
    "        #plt.show()\n",
    "\n",
    "        #plot_pacf(data_2.shift(lag))\n",
    "        #plt.show()\n",
    "\n",
    "        #y = data_1\n",
    "        #X = data_2\n",
    "        #reg = LinearRegression().fit(X, y)\n",
    "\n",
    "        #print(reg.score(X, y),reg.coef_,reg.intercept_)\n",
    "\n",
    "        #model = sm.OLS(y,X)\n",
    "        #results = model.fit()\n",
    "        #print(results.summary())\n",
    "\n",
    "        #Lagged_Differenced_Set = pd.concat([Lagged_Differenced_Set,data_2.shift(lag)],axis=1)\n",
    "        #TrainO_Lagged_Set = pd.concat([TrainO_Lagged_Set,all_data.iloc[:,f].shift(lag)],axis=1)\n",
    "        #if lag>0:\n",
    "            #lag = 0\n",
    "\n",
    "        Lagged_Set = pd.concat([Lagged_Set,og.iloc[:,f].shift(lag)],axis=1)\n",
    "\n",
    "        lagcorrs.append(lagc)\n",
    "\n",
    "        lags.append(lag)\n",
    "\n",
    "    Lagged_Set.index = set_.index\n",
    "    #stats = pd.concat([pd.DataFrame(set_.columns[1:]),pd.DataFrame(lags),pd.DataFrame(lagcorrs),pd.DataFrame(ogcorrs)],axis=1)\n",
    "    stats = pd.concat([pd.DataFrame(set_.columns[1:]),pd.DataFrame(lags),pd.DataFrame(ogcorrs)],axis=1)\n",
    "    \n",
    "    #print(Lagged_Set)\n",
    "    #return stats, pd.concat([data_1,Lagged_Set],axis=1),  pd.concat([data_1,Lagged_Differenced_Set],axis=1)\n",
    "    return stats, pd.concat([set_.iloc[:,0],Lagged_Set],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1497,
   "id": "23eb26a0-bb46-4fd9-b9b8-2de6a3a7f875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['^SP500TR', 'DGS10', 'DTB3', 'DGS3MO', 'MORTGAGE30US', 'DFII10',\n",
       "       'T5YIFR', 'BAMLHYH0A0HYM2TRIV', 'BAMLCC0A1AAATRIV', 'DGS1',\n",
       "       'BAMLCC0A4BBBTRIV', 'IC4WSA', 'WILLMICROCAPPR', 'WILLLRGCAPVAL',\n",
       "       'T5YIE', 'WTB3MS', 'WGS3MO', 'TWEXB', 'DEXCHUS', 'DEXUSUK', 'TEDRATE',\n",
       "       'VIXCLS', 'NFCI', 'BAA10Y', 'BAMLC0A0CM', 'BAMLH0A3HYC', 'DCOILBRENTEU',\n",
       "       'DCOILWTICO', 'DFF', 'DGS1MO', 'DGS30', 'DGS5', 'ICSA', 'M1', 'STLFSI2',\n",
       "       'T10Y2Y', 'T10Y3M', 'TREAST', 'QQQ', 'DIA', 'IYR', 'EWG', 'EWU', 'EWJ',\n",
       "       'EZU', 'EWZ', 'ILF', 'EWW', 'LQD', 'SHY', 'IEF', 'TLT', 'ETH', 'BCH',\n",
       "       'LTC', 'XLY', 'XLP', 'XLE', 'XLF', 'XLV', 'XLI', 'XLB', 'XLK', 'XLU',\n",
       "       'MMM', 'AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'GS',\n",
       "       'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', 'NKE',\n",
       "       'PG', 'TRV', 'UNH', 'VZ', 'WMT', 'WBA', 'DIS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1498,
   "id": "3e80173c-962f-4b0e-b170-8186d5f9b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748\n",
      "748\n"
     ]
    }
   ],
   "source": [
    "Lagged_Set = pd.DataFrame()\n",
    "Lagged_Differenced_Set = pd.DataFrame()\n",
    "\n",
    "#'''\n",
    "#Lagged_Differenced_Set_offset = pd.concat([out.result.iloc[:,0].pct_change().shift(-1),out.result.iloc[:,1:].pct_change()],axis=1)\n",
    "differenced_set = out.result.pct_change()\n",
    "differenced_set = differenced_set.replace([np.inf, -np.inf, np.NaN], 0)\n",
    "\n",
    "#ls_stats, Lagged_Differenced_Set= getLagged_Set(differenced_set, out.result)\n",
    "lso_stats, Lagged_Differenced_Set_offset= getLagged_Set(pd.concat([differenced_set.iloc[:,0].shift(-1),differenced_set.iloc[:,1:]],axis=1), out.result)\n",
    "\n",
    "#Lagged_Set_offset.index = out.result.index\n",
    "Lagged_Set_offset.dropna(inplace= True)\n",
    "#Lagged_Set_offset.columns = out.result.columns\n",
    "\n",
    "Lagged_Differenced_Set_offset.dropna(inplace= True)\n",
    "\n",
    "#Lagged_Differenced_Set.dropna(inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "57b19b33-dfd6-4233-8e3c-4f266a9ce8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0\n",
       "2003-09-30    0.060820\n",
       "2003-12-31    0.076878\n",
       "2004-03-31   -0.004395\n",
       "2004-06-30   -0.012870\n",
       "2004-09-30    0.057999\n",
       "2004-12-31    0.029813\n",
       "2005-03-31   -0.003927\n",
       "2005-06-30    0.040415\n",
       "2005-09-30    0.009843\n",
       "2005-12-31    0.048335\n",
       "Name: ^SP500TR, dtype: float64"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e943c-2c5b-4cc8-86ca-87a1dab05a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1500,
   "id": "29c83e18-ed0b-446d-89a3-86ea42e5672c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "^SP500TR        0\n",
       "AXP             0\n",
       "MMM             0\n",
       "XLU             0\n",
       "XLK             0\n",
       "               ..\n",
       "DCOILBRENTEU    0\n",
       "BAMLH0A3HYC     0\n",
       "BAMLC0A0CM      0\n",
       "NFCI            0\n",
       "DIS             0\n",
       "Length: 91, dtype: int64"
      ]
     },
     "execution_count": 1500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "temp = pd.DataFrame()\n",
    "temp = pd.concat([out.result.iloc[:,0].pct_change().shift(-1),Lagged_Set_offset.iloc[:,1:].pct_change()],axis=1).copy()\n",
    "Lagged_Differenced_Set_offset = pd.DataFrame()\n",
    "Lagged_Differenced_Set_offset = temp.copy()\n",
    "Lagged_Differenced_Set_offset.dropna(inplace= True)\n",
    "Lagged_Differenced_Set_offset\n",
    "Lagged_Differenced_Set_offset = Lagged_Differenced_Set_offset.replace([np.inf, -np.inf, np.NaN], 0)\n",
    "#Lagged_Differenced_Set_offset.fillna(0)\n",
    "#preprocessing.StandardScaler().fit(Lagged_Differenced_Set_offset)\n",
    "'''\n",
    "np.sum(Lagged_Differenced_Set_offset.isin([np.inf, -np.inf, np.NaN])).sort_values(kind=\"quicksort\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "id": "8cfbc252-b567-48e9-86f9-9e57df0ac45c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  0         0\n",
      "0    1  0  0.240905\n",
      "1    2  0  0.118558\n",
      "2    3  0  0.116696\n",
      "3    4  2  0.036341\n",
      "4    5  3 -0.081019\n",
      "..  .. ..       ...\n",
      "85  86  0  0.737366\n",
      "86  87  0  0.723140\n",
      "87  88  0  0.632875\n",
      "88  89  0  0.613970\n",
      "89  90  0  0.811703\n",
      "\n",
      "[90 rows x 3 columns]\n",
      "     0  0         0\n",
      "0    1  0 -0.152157\n",
      "1    2  2 -0.060111\n",
      "2    3  2 -0.060784\n",
      "3    4  1 -0.047973\n",
      "4    5  4  0.030118\n",
      "..  .. ..       ...\n",
      "85  86  3 -0.213698\n",
      "86  87  3 -0.199223\n",
      "87  88  4 -0.162327\n",
      "88  89  4 -0.167238\n",
      "89  90  4 -0.217201\n",
      "\n",
      "[90 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ls_stats)\n",
    "print(lso_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "53aa28e7-3712-466f-96fd-7bbf115714ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "^SP500TR        0\n",
       "AXP             0\n",
       "MMM             0\n",
       "XLU             0\n",
       "XLK             0\n",
       "               ..\n",
       "DCOILBRENTEU    0\n",
       "BAMLH0A3HYC     0\n",
       "BAMLC0A0CM      0\n",
       "NFCI            0\n",
       "DIS             0\n",
       "Length: 91, dtype: int64"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1501,
   "id": "abb40926-6979-4f9f-ad09-ed1f93c25a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The null hypothesis can be rejected\n",
      "10.12502733087351\n",
      "The null hypothesis can be rejected\n",
      "0.6964647905771697\n",
      "The null hypothesis can be rejected\n",
      "-0.8935173002890502\n",
      "The null hypothesis can be rejected\n",
      "-0.8830721574336661\n",
      "The null hypothesis can be rejected\n",
      "-0.12430665739736378\n",
      "The null hypothesis can be rejected\n",
      "0.940359230613309\n",
      "The null hypothesis can be rejected\n",
      "3.3571985158458357\n",
      "The null hypothesis can be rejected\n",
      "0.36220701351158063\n",
      "The null hypothesis can be rejected\n",
      "-0.23829453371775922\n",
      "The null hypothesis can be rejected\n",
      "-0.6951491095816904\n",
      "The null hypothesis can be rejected\n",
      "0.27471043868565914\n",
      "The null hypothesis can be rejected\n",
      "-6.941639661105911\n",
      "The null hypothesis can be rejected\n",
      "0.053719133495405365\n",
      "The null hypothesis can be rejected\n",
      "-6.941639661105911\n",
      "The null hypothesis can be rejected\n",
      "2.247767082400437\n",
      "The null hypothesis can be rejected\n",
      "-0.8903839179091455\n",
      "The null hypothesis can be rejected\n",
      "-0.8832982153703531\n",
      "The null hypothesis can be rejected\n",
      "-1.2088291600382113\n",
      "The null hypothesis can be rejected\n",
      "-5.0042714444867284\n",
      "The null hypothesis can be rejected\n",
      "-0.6095545747701068\n",
      "The null hypothesis can be rejected\n",
      "-3.9726416580592314\n",
      "The null hypothesis can be rejected\n",
      "-1.1679033029383572\n",
      "The null hypothesis can be rejected\n",
      "-1.8359372820653945\n",
      "The null hypothesis can be rejected\n",
      "-1.4440160526536037\n",
      "The null hypothesis can be rejected\n",
      "-2.254704280890903\n",
      "The null hypothesis can be rejected\n",
      "-0.8804337002541948\n",
      "The null hypothesis can be rejected\n",
      "0.3115742701219099\n",
      "The null hypothesis can be rejected\n",
      "0.43931632637931134\n",
      "The null hypothesis can be rejected\n",
      "-0.9017855779053471\n",
      "The null hypothesis can be rejected\n",
      "-0.9397540740353877\n",
      "The null hypothesis can be rejected\n",
      "1.0731944197692853\n",
      "The null hypothesis can be rejected\n",
      "0.15498920053197215\n",
      "The null hypothesis can be rejected\n",
      "-6.941639661105911\n",
      "The null hypothesis can be rejected\n",
      "-0.8995480466679928\n",
      "The null hypothesis can be rejected\n",
      "-0.52308630215543\n",
      "The null hypothesis can be rejected\n",
      "0.6778697894694311\n",
      "The null hypothesis can be rejected\n",
      "0.9481066774673932\n",
      "The null hypothesis can be rejected\n",
      "-6.941639661105911\n",
      "The null hypothesis can be rejected\n",
      "-0.47204597169818047\n",
      "The null hypothesis can be rejected\n",
      "-0.5489400940963144\n",
      "The null hypothesis can be rejected\n",
      "0.37814517675188775\n",
      "The null hypothesis can be rejected\n",
      "1.0409173155243099\n",
      "The null hypothesis can be rejected\n",
      "2.1139809827549296\n",
      "The null hypothesis can be rejected\n",
      "-0.24389931543065216\n",
      "The null hypothesis can be rejected\n",
      "0.8635332464628172\n",
      "The null hypothesis can be rejected\n",
      "1.038804818626338\n",
      "The null hypothesis can be rejected\n",
      "1.672210466284376\n",
      "The null hypothesis can be rejected\n",
      "1.7508991733732824\n",
      "The null hypothesis can be rejected\n",
      "0.03456047566439655\n",
      "The null hypothesis can be rejected\n",
      "4.608632464345892\n",
      "The null hypothesis can be rejected\n",
      "0.865015950179719\n",
      "The null hypothesis can be rejected\n",
      "0.03653651613084702\n",
      "The null hypothesis can be rejected\n",
      "1.7566682072424293\n",
      "The null hypothesis can be rejected\n",
      "0.6360811826961553\n",
      "The null hypothesis can be rejected\n",
      "0.38931665744971194\n",
      "The null hypothesis can be rejected\n",
      "-0.38126756671601486\n",
      "The null hypothesis can be rejected\n",
      "-0.20193308462231416\n",
      "The null hypothesis can be rejected\n",
      "1.4561683841579562\n",
      "The null hypothesis can be rejected\n",
      "0.488899798827083\n",
      "The null hypothesis can be rejected\n",
      "-0.5659176524219756\n",
      "The null hypothesis can be rejected\n",
      "-0.30437325918622227\n",
      "The null hypothesis can be rejected\n",
      "-0.040819227657396304\n",
      "The null hypothesis can be rejected\n",
      "-0.7083512006494838\n",
      "The null hypothesis can be rejected\n",
      "-0.10896957714335136\n",
      "The null hypothesis can be rejected\n",
      "-0.3626565172104222\n",
      "The null hypothesis can be rejected\n",
      "0.15935288488050278\n",
      "The null hypothesis can be rejected\n",
      "-0.4809843137158204\n",
      "The null hypothesis can be rejected\n",
      "0.05445326605181506\n",
      "The null hypothesis can be rejected\n",
      "-0.32797789198787203\n",
      "The null hypothesis can be rejected\n",
      "0.02938404668904844\n",
      "The null hypothesis can be rejected\n",
      "0.8577697992557481\n",
      "The null hypothesis can be rejected\n",
      "-1.0572226750524394\n",
      "The null hypothesis can be rejected\n",
      "0.12367680180850825\n",
      "The null hypothesis can be rejected\n",
      "0.2788432987589276\n",
      "The null hypothesis can be rejected\n",
      "-0.34503325691468634\n",
      "The null hypothesis can be rejected\n",
      "-0.21356826586271432\n",
      "The null hypothesis can be rejected\n",
      "1.2930201593919441\n",
      "The null hypothesis can be rejected\n",
      "-0.6996396329790758\n",
      "The null hypothesis can be rejected\n",
      "-0.5326160869807544\n",
      "The null hypothesis can be rejected\n",
      "-0.6953248834918493\n",
      "The null hypothesis can be rejected\n",
      "0.11284155910335841\n",
      "The null hypothesis can be rejected\n",
      "-0.2710059737544107\n",
      "The null hypothesis can be rejected\n",
      "-0.8711752590674099\n",
      "The null hypothesis can be rejected\n",
      "-0.16003298134274285\n",
      "The null hypothesis can be rejected\n",
      "-0.5982389809829709\n",
      "The null hypothesis can be rejected\n",
      "-0.11147231982630866\n",
      "The null hypothesis can be rejected\n",
      "-0.3474963025173841\n",
      "The null hypothesis can be rejected\n",
      "-0.08577242865010691\n",
      "The null hypothesis can be rejected\n",
      "-0.9088834618099401\n",
      "The null hypothesis can be rejected\n",
      "-0.41925235447334913\n",
      "The null hypothesis can be rejected\n",
      "-0.16992332953699446\n"
     ]
    }
   ],
   "source": [
    "transformed, lambdas = transform_boxcox(Lagged_Differenced_Set_offset.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1422b2e-d8e4-43aa-a691-8c54392a6291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72722f-fbff-4bff-92b8-26b19888e415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "8cc90237-f851-4afb-86f8-2dca0b41123d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cbd = True\n",
    "ic = True\n",
    "\n",
    "# evaluate an elastic net model on the dataset\n",
    "tsize = .20\n",
    "#train, test = train_test_split(Lagged_Differenced_Set_offset.iloc[:,0:], test_size=tsize, shuffle=False)\n",
    "train, test = train_test_split(transformed.iloc[:,0:], test_size=tsize, shuffle=False)\n",
    "\n",
    "interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=ic)\n",
    "\n",
    "#train_t, lambdas_t = transform_boxcox(train)\n",
    "\n",
    "#disabled boxcox\n",
    "if cbd:\n",
    "    train_t = train\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_t)\n",
    "\n",
    "#\n",
    "train_s = pd.DataFrame(scaler.transform(train_t))\n",
    "train_s.columns = train.columns\n",
    "train_s.index = train.index  \n",
    "\n",
    "train_t = train_s\n",
    "\n",
    "#test_t = transform_boxcox_l(test, lambdas_t)\n",
    "\n",
    "#disabled boxcox\n",
    "if cbd:\n",
    "    test_t = test\n",
    "\n",
    "test_s = pd.DataFrame(scaler.transform(test_t))\n",
    "test_s.columns = test.columns\n",
    "test_s.index = test.index\n",
    "\n",
    "test_t = test_s\n",
    "\n",
    "y_train = pd.DataFrame(train_t.iloc[:,0])\n",
    "\n",
    "#exclude y\n",
    "\n",
    "X_inter_train = pd.DataFrame(interaction.fit_transform(train_t.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(train_t.iloc[:,1:]).columns))\n",
    "\n",
    "    #apply ZCA each time a set of factors are removed (i.e. iteratively)\n",
    " #trf = zca.ZCA().fit(X_inter_train)\n",
    "  #trf = zca.ZCA().fit(X_train)\n",
    "\n",
    " #X_train = pd.DataFrame(trf.transform(X_inter_train))\n",
    "  #X_train = pd.DataFrame(trf.transform(X_train))\n",
    " #X_train.columns=X_inter_train.columns\n",
    "  #X_train.columns=X_train.columns\n",
    "  #X_train.index = train.index\n",
    "\n",
    "#X_inter_alt = X_train.iloc[:, np.array(range(0,len(all_data.iloc[:,2:].columns)))]\n",
    "#print(X_inter_alt.head(3))\n",
    "\n",
    "y_test = pd.DataFrame(test_t.iloc[:,0])\n",
    "\n",
    "X_inter_test = pd.DataFrame(interaction.fit_transform(test_t.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(test_t.iloc[:,1:]).columns))\n",
    "\n",
    " #X_test = pd.DataFrame(trf.transform(X_inter_test))\n",
    "  #X_test = pd.DataFrame(trf.transform(X_test))\n",
    "\n",
    " #X_test.columns=X_inter_test.columns\n",
    "  #X_test.columns=X_test.columns\n",
    "  #X_test.index = test.index\n",
    "\n",
    "#X_inter_t_alt = X_test.iloc[:, np.array(range(0,len(all_data.iloc[:,2:].columns)))]\n",
    "#X_inter_t_alt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf46d0-1162-46d6-a166-0461b321e0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ratios = arange(0, 1, 0.01)\n",
    "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "\n",
    " #model = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=4, verbose=0, precompute='auto')\n",
    "\n",
    "model = ElasticNet()\n",
    "grid = dict()\n",
    "# fit model\n",
    "\n",
    "grid['alpha'] = alphas\n",
    "grid['l1_ratio'] = ratios\n",
    "\n",
    "#search = HalvingRandomSearchCV(model, grid,resource='n_samples',max_resources=10,random_state=0)\n",
    "\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    " #results = search.fit(X_inter_train, y_train)\n",
    "#results = search.fit(X_train, y_train)\n",
    "\n",
    "results = search.fit(X_inter_train, y_train)\n",
    "\n",
    " #model.fit(X_inter_train, y_train)\n",
    "# summarize chosen configuration\n",
    "\n",
    "print('MAE: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)\n",
    "\n",
    "print(results.best_estimator_)\n",
    "best_model = ElasticNet(alpha=results.best_estimator_.alpha, l1_ratio = results.best_estimator_.l1_ratio)\n",
    "\n",
    "#pd.concat([all_data[Y],all_data_int],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb7965-c50b-4f1e-bd5e-05504a69192f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model.fit(X_inter_train,train_t.iloc[:,0])\n",
    "\n",
    "trainScore = best_model.score(X_inter_train, y_train, sample_weight=None)\n",
    "testScore = best_model.score(X_inter_test, y_test, sample_weight=None)\n",
    "print(trainScore)\n",
    "print(testScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb694fa2-0e07-4a01-8648-2d6615a70b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "093a64f9-522d-4761-adf1-f55d566a1b70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b12fd-2b69-4243-909a-6f828aa43b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "id": "0e86999a-16ba-4505-b5ec-843d219ac62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.538462\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1067,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367906e7-5aec-4529-8f29-42040e9ac43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1502,
   "id": "14400790-a56a-4084-8c56-cf73697546e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00359930463040558\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-1.1932117065240826, pvalue=0.23385351361428686)\n",
      "0.0021067462467523747\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-0.790416494004862, pvalue=0.4300247851777337)\n"
     ]
    }
   ],
   "source": [
    "#classification transforms\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE \n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "#set_ = Lagged_Differenced_Set_offset.copy()\n",
    "#set_ = transformed.copy()\n",
    "#DF_list = list()\n",
    "\n",
    "sets = list()\n",
    "sets.append(transformed.copy())\n",
    "sets.append(Lagged_Differenced_Set_offset.copy())\n",
    "\n",
    "def return_sets(sets):    \n",
    "    \n",
    "    sets_outer = list()\n",
    "    \n",
    "    for i in sets:\n",
    "        \n",
    "        sets_multiple = list()\n",
    "        \n",
    "        set_ = pd.DataFrame()\n",
    "        set_ = i\n",
    "\n",
    "        X, y = set_.iloc[:,1:],  set_.iloc[:,0]\n",
    "        \n",
    "        print(mean(y))\n",
    "\n",
    "        y = pd.DataFrame(np.where(y > mean(y), 1, 0))\n",
    "        y.index = set_.index\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=tsize, shuffle=False)\n",
    "\n",
    "        a = set_.iloc[:,0].loc[X_train.index]\n",
    "        b = set_.iloc[:,0].loc[X_test.index]\n",
    "        plt.hist(a)\n",
    "        plt.show()\n",
    "        plt.hist(b)\n",
    "        plt.show()\n",
    "        print(stats.ttest_ind(a,b, equal_var = False))\n",
    "\n",
    "        one = y_train[y_train==0].dropna().sample(int(len(X_train)*2), replace=True, random_state=1)\n",
    "        two = y_train[y_train==1].dropna().sample(int(len(X_train)*2), replace=True, random_state=1)\n",
    "\n",
    "        train_index = np.append(one.index,two.index)\n",
    "\n",
    "        #destroyed my data\n",
    "        #smote = SVMSMOTE(random_state=42)\n",
    "            #smote = SMOTE()\n",
    "        #Xsm_train, ysm_train = smote.fit_sample(np.array(X_train),np.array(y_train))\n",
    "\n",
    "        Xsm_train = X_train.copy()\n",
    "        ysm_train = y_train.copy()\n",
    "        \n",
    "        #use with optimal lags\n",
    "        y_test = y_test.loc[y_test.index>y_test.index[maxl]]\n",
    "        X_test = X_test.loc[X_test.index>X_test.index[maxl]]\n",
    "\n",
    "        scaler = preprocessing.StandardScaler().fit(Xsm_train)\n",
    "\n",
    "        X_train_transformed = pd.DataFrame(scaler.transform(Xsm_train))\n",
    "        X_train_transformed.columns = X_train.columns\n",
    "\n",
    "        X_inter_train = X_train_transformed.copy()\n",
    "\n",
    "        #X_inter_train = pd.DataFrame(interaction.fit_transform(X_train_transformed), columns=interaction.get_feature_names(input_features=pd.DataFrame(X_train_transformed).columns))\n",
    "\n",
    "        trf = zca.ZCA().fit(X_inter_train)\n",
    "\n",
    "        #X_inter_train = pd.DataFrame(trf.transform(X_inter_train))\n",
    "        X_inter_train.columns=pd.DataFrame(X_inter_train).columns\n",
    "        #X_inter_train.index = X_train.loc[train_index].index\n",
    "\n",
    "        X_test_transformed = pd.DataFrame(scaler.transform(X_test))\n",
    "        X_test_transformed.columns = X_test.columns\n",
    "        X_test_transformed.index = X_test.index\n",
    "\n",
    "        X_inter_test = X_test_transformed\n",
    "\n",
    "        #X_inter_test = pd.DataFrame(interaction.fit_transform(X_test_transformed), columns=interaction.get_feature_names(input_features=pd.DataFrame(X_test_transformed).columns))\n",
    "\n",
    "        #X_inter_test = pd.DataFrame(trf.transform(X_inter_test))\n",
    "\n",
    "        sets_multiple.append(X_inter_train)\n",
    "        sets_multiple.append(y_train)\n",
    "        sets_multiple.append(X_inter_test)\n",
    "        sets_multiple.append(y_test)        \n",
    "    \n",
    "        sets_outer.append(sets_multiple)\n",
    "        \n",
    "    return(sets_outer)\n",
    "\n",
    "values = return_sets(sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568baa66-4a53-4f23-ba8a-c5f9408f1591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "id": "73ec310d-7f86-4154-be9b-11c927a6a4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1503,
   "id": "e560215f-9b7d-4d76-ac61-dbf96d2cfd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48 accuracy with a standard deviation of 0.04\n",
      "0.4222222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      1.00      0.58        73\n",
      "           1       1.00      0.03      0.05       107\n",
      "\n",
      "    accuracy                           0.42       180\n",
      "   macro avg       0.71      0.51      0.32       180\n",
      "weighted avg       0.76      0.42      0.27       180\n",
      "\n",
      "[[ 73   0]\n",
      " [104   3]]\n",
      "[1] 744\n",
      "180\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      1.00      0.58        73\n",
      "           1       1.00      0.03      0.05       107\n",
      "\n",
      "    accuracy                           0.42       180\n",
      "   macro avg       0.71      0.51      0.32       180\n",
      "weighted avg       0.76      0.42      0.27       180\n",
      "\n",
      "[[ 73   0]\n",
      " [104   3]]\n",
      "0.49 accuracy with a standard deviation of 0.08\n",
      "0.6111111111111112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        70\n",
      "           1       0.61      1.00      0.76       110\n",
      "\n",
      "    accuracy                           0.61       180\n",
      "   macro avg       0.31      0.50      0.38       180\n",
      "weighted avg       0.37      0.61      0.46       180\n",
      "\n",
      "[[  0  70]\n",
      " [  0 110]]\n",
      "[1] 744\n",
      "180\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        70\n",
      "           1       0.61      0.99      0.75       110\n",
      "\n",
      "    accuracy                           0.61       180\n",
      "   macro avg       0.30      0.50      0.38       180\n",
      "weighted avg       0.37      0.61      0.46       180\n",
      "\n",
      "[[  0  70]\n",
      " [  1 109]]\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "#works best\n",
    "#class balanced, no zca\n",
    "from sklearn import svm\n",
    "\n",
    "for i in range(0,len(sets)):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "\n",
    "    clf = svm.SVC(kernel='rbf', C=1, random_state=42)\n",
    "\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "    clf = svm.SVC(C=1).fit(X_train, y_train)\n",
    "\n",
    "    train_predict = clf.predict(X_train)\n",
    "    predicteds = clf.predict(X_test)\n",
    "    #predicted.index = y_test.index\n",
    "\n",
    "    print(clf.score(X_test, y_test))\n",
    "\n",
    "    #clf.predict(X_test)\n",
    "    #print(predicteds)\n",
    "    print(metrics.classification_report(y_test,predicteds))\n",
    "    print(metrics.confusion_matrix(y_test,predicteds))\n",
    "\n",
    "    predictions_s = arfima_adjust(y_train, y_test, train_predict, predicteds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "id": "ec38a51f-ede1-47bb-9af7-ee25b6106160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25944973-ee0b-4b74-be6c-f59463229bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39fe4e-d604-4fa9-ad81-ee7a53a06784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "3909eb8a-645f-47c5-933a-98d61f3d6ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 815,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('svc', svm.SVC(kernel='sigmoid', C=1, random_state=42))])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)\n",
    "'''\n",
    "#search = cross_val_score(pipe, X_inter_train, y_train.loc[train_index], cv=10)\n",
    "#GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "#search.fit(X_digits, y_digits)\n",
    "#print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "#print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "id": "d83b6ec4-6148-41ba-9d1b-3765f913ee93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 1045,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1505,
   "id": "b49b4466-cd31-4f19-85e0-c32db62dbe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.64      0.51        70\n",
      "           1       0.67      0.45      0.54       110\n",
      "\n",
      "    accuracy                           0.53       180\n",
      "   macro avg       0.55      0.55      0.53       180\n",
      "weighted avg       0.57      0.53      0.53       180\n",
      "\n",
      "[[45 25]\n",
      " [60 50]]\n",
      "[1] 744\n",
      "180\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.64      0.51        70\n",
      "           1       0.67      0.45      0.54       110\n",
      "\n",
      "    accuracy                           0.53       180\n",
      "   macro avg       0.55      0.55      0.53       180\n",
      "weighted avg       0.57      0.53      0.53       180\n",
      "\n",
      "[[45 25]\n",
      " [60 50]]\n",
      "LogisticRegression()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.03      0.05        73\n",
      "           1       0.60      0.98      0.74       107\n",
      "\n",
      "    accuracy                           0.59       180\n",
      "   macro avg       0.55      0.50      0.40       180\n",
      "weighted avg       0.56      0.59      0.46       180\n",
      "\n",
      "[[  2  71]\n",
      " [  2 105]]\n",
      "[1] 744\n",
      "180\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.03      0.05        73\n",
      "           1       0.60      0.98      0.74       107\n",
      "\n",
      "    accuracy                           0.59       180\n",
      "   macro avg       0.55      0.50      0.40       180\n",
      "weighted avg       0.56      0.59      0.46       180\n",
      "\n",
      "[[  2  71]\n",
      " [  2 105]]\n"
     ]
    }
   ],
   "source": [
    "#Logistic1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "for i in reversed(range(0,len(sets))):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "\n",
    "    modell = LogisticRegression()\n",
    "    modell.fit(X_train, y_train)\n",
    "    print(modell)\n",
    "\n",
    "    #expected = y_test\n",
    "    train_predict = modell.predict(X_train)\n",
    "    predictedl = modell.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test,predictedl))\n",
    "    print(metrics.confusion_matrix(y_test,predictedl))\n",
    "    \n",
    "    predictions_l = arfima_adjust(y_train, y_test, train_predict, predictedl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "id": "7672895b-f442-45b4-8cb1-3257e8c830a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.96      0.56        70\n",
      "           1       0.67      0.05      0.10       110\n",
      "\n",
      "    accuracy                           0.41       180\n",
      "   macro avg       0.53      0.51      0.33       180\n",
      "weighted avg       0.56      0.41      0.28       180\n",
      "\n",
      "[[ 67   3]\n",
      " [104   6]]\n",
      "[1] 744\n",
      "180\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.96      0.56        70\n",
      "           1       0.67      0.05      0.10       110\n",
      "\n",
      "    accuracy                           0.41       180\n",
      "   macro avg       0.53      0.51      0.33       180\n",
      "weighted avg       0.56      0.41      0.28       180\n",
      "\n",
      "[[ 67   3]\n",
      " [104   6]]\n",
      "GaussianNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      1.00      0.58        73\n",
      "           1       0.00      0.00      0.00       107\n",
      "\n",
      "    accuracy                           0.41       180\n",
      "   macro avg       0.20      0.50      0.29       180\n",
      "weighted avg       0.16      0.41      0.23       180\n",
      "\n",
      "[[ 73   0]\n",
      " [107   0]]\n",
      "[1] 744\n",
      "180\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      1.00      0.58        73\n",
      "           1       0.00      0.00      0.00       107\n",
      "\n",
      "    accuracy                           0.41       180\n",
      "   macro avg       0.20      0.50      0.29       180\n",
      "weighted avg       0.16      0.41      0.23       180\n",
      "\n",
      "[[ 73   0]\n",
      " [107   0]]\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "for i in reversed(range(0,len(sets))):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "\n",
    "    modeln = GaussianNB()\n",
    "    modeln.fit(X_train, y_train)\n",
    "    print(modeln)\n",
    "\n",
    "    #expected = y_test\n",
    "    train_predict = modeln.predict(X_train)\n",
    "    predictedn = modeln.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test,predictedn))\n",
    "    print(metrics.confusion_matrix(y_test,predictedn))\n",
    "    \n",
    "    predictions_n = arfima_adjust(y_train, y_test, train_predict, predictedn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "id": "644b63d8-9f6e-453e-b896-0ee262d1242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.70      0.52        73\n",
      "           1       0.62      0.34      0.44       107\n",
      "\n",
      "    accuracy                           0.48       180\n",
      "   macro avg       0.52      0.52      0.48       180\n",
      "weighted avg       0.54      0.48      0.47       180\n",
      "\n",
      "[[51 22]\n",
      " [71 36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.70      0.52        73\n",
      "           1       0.62      0.34      0.44       107\n",
      "\n",
      "    accuracy                           0.48       180\n",
      "   macro avg       0.52      0.52      0.48       180\n",
      "weighted avg       0.54      0.48      0.47       180\n",
      "\n",
      "[[51 22]\n",
      " [71 36]]\n",
      "DecisionTreeClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.80      0.55        70\n",
      "           1       0.70      0.29      0.41       110\n",
      "\n",
      "    accuracy                           0.49       180\n",
      "   macro avg       0.56      0.55      0.48       180\n",
      "weighted avg       0.59      0.49      0.46       180\n",
      "\n",
      "[[56 14]\n",
      " [78 32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.80      0.55        70\n",
      "           1       0.70      0.29      0.41       110\n",
      "\n",
      "    accuracy                           0.49       180\n",
      "   macro avg       0.56      0.55      0.48       180\n",
      "weighted avg       0.59      0.49      0.46       180\n",
      "\n",
      "[[56 14]\n",
      " [78 32]]\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier\n",
    "#works best with transformed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "for i in range(0,len(sets)):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "\n",
    "    modeld = DecisionTreeClassifier()\n",
    "    modeld.fit(X_train, y_train)\n",
    "    print(modeld)\n",
    "\n",
    "    #expected = y_test\n",
    "    train_predict = modeld.predict(X_train)\n",
    "    predictedd = modeld.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test,predictedd))\n",
    "    print(metrics.confusion_matrix(y_test,predictedd))\n",
    "        \n",
    "    predictions_d = arima_adjust(y_train, y_test, train_predict, predictedd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "id": "6ce0671f-8931-4f7e-936b-51adcb52dab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 21}\n",
      "0.5040268456375838\n",
      "[[69  4]\n",
      " [98  9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.95      0.57        73\n",
      "           1       0.69      0.08      0.15       107\n",
      "\n",
      "    accuracy                           0.43       180\n",
      "   macro avg       0.55      0.51      0.36       180\n",
      "weighted avg       0.58      0.43      0.32       180\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.95      0.57        73\n",
      "           1       0.69      0.08      0.15       107\n",
      "\n",
      "    accuracy                           0.43       180\n",
      "   macro avg       0.55      0.51      0.36       180\n",
      "weighted avg       0.58      0.43      0.32       180\n",
      "\n",
      "[[69  4]\n",
      " [98  9]]\n",
      "{'n_neighbors': 23}\n",
      "0.4851804824959188\n",
      "[[  1  69]\n",
      " [  2 108]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.01      0.03        70\n",
      "           1       0.61      0.98      0.75       110\n",
      "\n",
      "    accuracy                           0.61       180\n",
      "   macro avg       0.47      0.50      0.39       180\n",
      "weighted avg       0.50      0.61      0.47       180\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.01      0.03        70\n",
      "           1       0.61      0.98      0.75       110\n",
      "\n",
      "    accuracy                           0.61       180\n",
      "   macro avg       0.47      0.50      0.39       180\n",
      "weighted avg       0.50      0.61      0.47       180\n",
      "\n",
      "[[  1  69]\n",
      " [  2 108]]\n"
     ]
    }
   ],
   "source": [
    "#knn\n",
    "#works best with transformed\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV#create new a knn model\n",
    "\n",
    "for i in range(0,len(sets)):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "    \n",
    "    knn2 = KNeighborsClassifier(metric='euclidean',weights='distance')#create a dictionary of all values we want to test for n_neighbors\n",
    "    param_grid = {'n_neighbors': np.arange(1, 25)}#use gridsearch to test all values for n_neighbors\n",
    "    knn_gscv = GridSearchCV(knn2, param_grid, cv=5)#fit model to data\n",
    "    knn_gscv.fit(X_train, y_train)\n",
    "    #check top performing n_neighbors value\n",
    "    print(knn_gscv.best_params_)\n",
    "    #check mean score for the top performing value of n_neighbors\n",
    "    print(knn_gscv.best_score_)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors = knn_gscv.best_params_['n_neighbors'],metric='euclidean',weights='distance')\n",
    "    knn.fit(X_train,y_train)\n",
    "    train_predict = knn.predict(X_train)\n",
    "    predictedk = knn.predict(X_test)\n",
    "    knn.score(X_test, y_test)\n",
    "\n",
    "    print(confusion_matrix(y_test, predictedk))\n",
    "    print(classification_report(y_test, predictedk))\n",
    "    \n",
    "    predictions_k = arima_adjust(y_train, y_test, train_predict, predictedk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43107e46-54bb-4837-a798-af32efaccadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "e55c7caf-0c3b-4911-ba58-8313a7ba85ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-771-d7e137a3e15d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msearch_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mresults_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_inter_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mysm_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m  \u001b[0;31m#model.fit(X_inter_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/distvol/Python-3.9.4/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/distvol/Python-3.9.4/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/distvol/Python-3.9.4/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/distvol/Python-3.9.4/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/distvol/Python-3.9.4/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/distvol/Python-3.9.4/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/distvol/Python-3.9.4/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/distvol/Python-3.9.4/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/distvol/Python-3.9.4/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Elastic logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "# define model\n",
    "ratios = arange(0, 1, 0.01)\n",
    "alphas = (10 ** np.linspace(-1, 1, 10))/10\n",
    "alphas = np.append(alphas,[10.0, 100.0])\n",
    "\n",
    "model = SGDClassifier(loss=\"log\", penalty=\"elasticnet\")\n",
    "grid = dict()\n",
    "# fit model\n",
    "\n",
    "grid['alpha'] = alphas\n",
    "grid['l1_ratio'] = ratios\n",
    "\n",
    "search_l = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "results_l = search_l.fit(X_inter_train, ysm_train)\n",
    "\n",
    " #model.fit(X_inter_train, y_train)\n",
    "# summarize chosen configuration\n",
    "\n",
    "print('MAE: %.3f' % results_l.best_score_)\n",
    "print('Config: %s' % results_l.best_params_)\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "print(results_l.best_estimator_)\n",
    "best_model_l = SGDClassifier(alpha=results_l.best_estimator_.alpha, l1_ratio = results_l.best_estimator_.l1_ratio)\n",
    "\n",
    "#calibrator = CalibratedClassifierCV(best_model_l, cv='prefit')\n",
    "#model=calibrator.fit(X_inter_train, y_train)\n",
    "\n",
    "best_model_l.fit(X_inter_train,ysm_train)\n",
    "\n",
    "#sgd = SGDClassifier()\n",
    "\n",
    "#sgd.partial_fit(X_inter_train,y_train, classes=[0,1])\n",
    "\n",
    "print(pd.DataFrame(np.transpose(best_model_l.coef_)).set_index(X_inter_train.columns))\n",
    "\n",
    "trainScore_l = best_model_l.score(X_inter_train, ysm_train, sample_weight=None)\n",
    "testScore_l = best_model_l.score(X_inter_test, y_test, sample_weight=None)\n",
    "print(trainScore_l)\n",
    "print(testScore_l)\n",
    "\n",
    "#print(help(KNeighborsClassifier))\n",
    "predictedl = best_model_l.predict(X_inter_test)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b9087-fe22-4d54-b152-a40e48784774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015d3fd-f35c-4f6c-b8e9-f66c92514715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "id": "a6dd1212-7817-4f74-95d6-0493d68c9658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  68]\n",
      " [  2 108]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.03      0.05        70\n",
      "           1       0.61      0.98      0.76       110\n",
      "\n",
      "    accuracy                           0.61       180\n",
      "   macro avg       0.56      0.51      0.40       180\n",
      "weighted avg       0.57      0.61      0.48       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#predictedf = pd.concat([pd.DataFrame(predicteds),pd.DataFrame(predictedk),pd.DataFrame(predictedl)],axis=1)\n",
    "predictedf = pd.concat([pd.DataFrame(predictions_s),pd.DataFrame(predictions_k),pd.DataFrame(predictions_d),pd.DataFrame(predictions_l),pd.DataFrame(predictions_n)],axis=1)\n",
    "predictedf.columns = [\"s\",\"k\",\"d\",\"l\",\"n\"]\n",
    "predictedf.index = y_test.index\n",
    "#print(predictedf)\n",
    "print(confusion_matrix(y_test, predictedf.mode(axis=1).iloc[:,0]))\n",
    "#print(confusion_matrix(y_test, predictedf.median(axis=1)))\n",
    "print(classification_report(y_test, predictedf.mode(axis=1).iloc[:,0]))\n",
    "#print(classification_report(y_test, predictedf.median(axis=1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4dfc37-ab80-427f-bc37-edb9a212c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate output multi-step 1d cnn example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dense, SimpleRNN, GRU, LSTM\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Lagged_Differenced_Set = (all_data - all_data.shift(-1)).dropna()\n",
    "\n",
    "def plot_learning_curves(loss, val_loss):\n",
    "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.\", label=\"Training loss\")\n",
    "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r-\", label=\"Validation loss\")\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out, xcolumns, ycolumns):\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, xcolumns], sequences[end_ix:out_end_ix, ycolumns]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return array(X), array(y)\n",
    "\n",
    "#n_steps_in = int(np.floor(len(train)*.8))\n",
    "print(len(Lagged_Differenced_Set))\n",
    "n_steps_in = int(np.round(len(Lagged_Differenced_Set)/2))\n",
    "print(len(Lagged_Differenced_Set) - n_steps_in)\n",
    "print(n_steps_in)\n",
    "#n_steps_out = int(len(train)-n_steps_in)\n",
    "n_steps_out = 1\n",
    "print(n_steps_out)\n",
    "\n",
    "#ycolumns = range(0,len(transformed.columns))\n",
    "ycolumns = range(0,len(Lagged_Differenced_Set.columns[0:1].values))\n",
    "xcolumns = range(1,len(Lagged_Differenced_Set.columns[1:]))\n",
    "\n",
    "#trainInner, valid = train_test_split(trainOuter, test_size=0.15, shuffle=False)\n",
    "#trainInner_whitened = pd.DataFrame(trf.transform(trainInner.iloc[:,1:])).set_index(trainInner.index)\n",
    "#valid_whitened = pd.DataFrame(trf.transform(valid.iloc[:,1:])).set_index(valid.index)\n",
    "#test_whitened = pd.DataFrame(trf.transform(test.iloc[:,1:])).set_index(test.index)\n",
    "\n",
    "trainOuter_whitened=pd.DataFrame(trf.transform(trainOuter.iloc[:,1:])).set_index(trainOuter.index)\n",
    "\n",
    "X, y = split_sequences(np.array(Lagged_Differenced_Set), n_steps_in, n_steps_out, xcolumns, ycolumns)\n",
    "\n",
    "#for i in range(len(X)):\n",
    "#    print(X[i], y[i])\n",
    "    \n",
    "#flatten output\n",
    "\n",
    "if len(ycolumns) > 0:\n",
    "    n_output = y.shape[1] * y.shape[2]\n",
    "    y = y.reshape((y.shape[0], n_output))\n",
    "\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model\n",
    "modelCNN = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(n_output)\n",
    "])\n",
    "\n",
    "# fit model\n",
    "#model.fit(X, y, epochs=7000, verbose=0)\n",
    "\n",
    "epochs_ = 200\n",
    "batch_size_ = 25\n",
    "\n",
    "#np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam', loss='mse')\n",
    "modelCNN.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "\n",
    "#model6.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=tsize,\n",
    "                                                    shuffle = False)\n",
    "                                                    #random_state=42)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_transformed = pd.DataFrame(scaler.transform(X_train))\n",
    "X_train_transformed.columns = X_train.columns\n",
    "\n",
    "X_test_transformed = pd.DataFrame(scaler.transform(X_test))\n",
    "X_test_transformed.columns = X_test.columns\n",
    "X_test_transformed.index = X_test.index\n",
    "\n",
    "    #new_series = pd.DataFrame(ts_train_scaled).append(pd.DataFrame(ts_valid_scaled)).append(pd.DataFrame(ts_test_scaled))\n",
    "\n",
    "    #series_reshaped = np.array([new_series[i:i + (n_steps+n_ahead)].copy() for i in range(len(data) - (n_steps+n_ahead))])  \n",
    "    \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,\n",
    "                                                    test_size=tsize,\n",
    "                                                    shuffle = False)\n",
    "                                                    #random_state=42)    \n",
    "\n",
    "#model6.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "historyCNN = modelCNN.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,validation_data=(X_valid, y_valid), verbose=0)\n",
    "#history6 = model6.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,validation_data=(X_valid, y_valid), verbose=0)\n",
    "#history = model.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f7e2d-3603-4ba1-8457-4fd9918498a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "plot_learning_curves(historyCNN.history[\"loss\"], history.history[\"val_loss\"])\n",
    "plt.show()\n",
    "#plot_learning_curves(history6.history[\"loss\"], history.history[\"val_loss\"])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f6d94-9757-4baa-b095-8937fbba5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4c952-450d-43d4-86c1-2a37673a58d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc3da-fa1f-4e57-9d84-f4e5f8872af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#multi\n",
    "yhat = modelCNN.predict(X_test, verbose=0)\n",
    "\n",
    "predicted = []\n",
    "original = []\n",
    "if len(ycolumns) > 1:\n",
    "    results = yhat.reshape(len(y_test), int(yhat.shape[1]/X_test.shape[2]), len(transformed.columns))\n",
    "    for i in range(0,len(results)):\n",
    "        predicted.append(pd.DataFrame(results[i])[0][0])\n",
    "        original.append(pd.DataFrame(y_test[i])[0][0])\n",
    "        #print(pd.DataFrame(results[i])[0][0])\n",
    "        #print(pd.DataFrame(y_test[i])[0][0])\n",
    "else:\n",
    "    results = yhat\n",
    "    for i in range(0,len(results)):\n",
    "        predicted.append(results[i])\n",
    "        original.append(y_test[i])\n",
    "        #print(pd.DataFrame(results[i]))\n",
    "        #print(pd.DataFrame(y_test[i]))\n",
    "        \n",
    "plt.scatter(predicted,original)\n",
    "\n",
    "pd.concat([pd.DataFrame(predicted),pd.DataFrame(original)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8229628-eed4-4b76-831d-1de033240347",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(best_model.coef_).set_index(X_inter_train.columns)\n",
    "\n",
    "a_coef = abs(coef)\n",
    "a_coef.sort_values(by=[0],ascending=False,inplace=True)\n",
    "chosen_few = a_coef[a_coef>0].dropna().index.values\n",
    "\n",
    "scaler_ = preprocessing.StandardScaler().fit(transformed)\n",
    "\n",
    "#\n",
    "train_ = pd.DataFrame(scaler_.transform(transformed))\n",
    "train_.columns = transformed.columns\n",
    "train_.index = transformed.index  \n",
    "\n",
    "X_inter_train_ = pd.DataFrame(interaction.fit_transform(train_.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(train_.iloc[:,1:]).columns))\n",
    "\n",
    "max_pvalue = 1\n",
    "New_Names = X_inter_train.columns\n",
    "X_b = X_inter_train_[chosen_few]\n",
    "while (max_pvalue > .05):\n",
    "        \n",
    "    trf = zca.ZCA().fit(X_b)\n",
    "        \n",
    "    X_b_z = pd.DataFrame(trf.transform(X_b))\n",
    "    X_b_z.columns=pd.DataFrame(X_b).columns\n",
    "    X_b_z.index = train_.index\n",
    "\n",
    "    model_ = sm.OLS(train_.iloc[:,0],sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip'))        \n",
    "    #model_ = sm.OLS(pd.DataFrame(pd.concat([train_.iloc[:,0],X_b_z],axis=1),sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip'))        \n",
    "    results_ = model_.fit()\n",
    "\n",
    "    set_ = X_b.columns.tolist()\n",
    "    \n",
    "    max_pvalue = max(results_.pvalues[1:])\n",
    "    if (max_pvalue > .05):\n",
    "        print(max_pvalue)\n",
    "        max_pname = (results_.pvalues[1:]).idxmax(axis=1)\n",
    "        set_.remove(max_pname)\n",
    "        New_Names = set_\n",
    "    \n",
    "        X_b = X_inter_train_[New_Names]\n",
    "        X_b.index = X_inter_train_.index\n",
    "\n",
    "#from statsmodels.formula.api import ols\n",
    "#lm = ols(pd.DataFrame(train_.iloc[:,0]) ~ sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip')).fit()\n",
    "#table = sm.stats.anova_lm(model_, type=3)\n",
    "#print(table)\n",
    "print(results_.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e428c-cce9-46e3-b145-754fc9e4c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNaNCols(df_):\n",
    "    for col in df_:        \n",
    "        num_NaNs = df_[col].isnull().sum()\n",
    "        if num_NaNs > 0:\n",
    "            print(f\"Column: {col}\")\n",
    "            print(f\"Number of NaNs: {num_NaNs}\")\n",
    "\n",
    "findNaNCols(X_b_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4e220-4ac2-4945-9e3a-4b8ff1355a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d940860-86d6-4768-81a0-63f9127143a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_plot = Plot.LinearRegressionResidualPlot(X_b_z, pd.DataFrame(train_.iloc[:,0]))\n",
    "lm = linear_plot.fit()\n",
    "summary, diag_res = linear_plot.diagnostic_plots(lm)\n",
    "print(\"Summary of Regression\\n:{}\".format(summary))\n",
    "print(\"Diagnostic Tests of Regression\\n:{}\".format(diag_res))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#sns.pairplot(pd.concat([pd.DataFrame(train[Y]),X_b_z],axis=1), hue=Y, height=2);\n",
    "\n",
    "pd.concat([pd.DataFrame(train[Y]),X_b_z],axis=1).hist()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model_s = sklearn.linear_model.LinearRegression()\n",
    "model_s.fit(X_b_z, pd.DataFrame(train_[Y]))\n",
    "\n",
    "shap.initjs()\n",
    "e = shap.explainers.Linear(model_s, X_b_z)\n",
    "\n",
    "shap_values = e.shap_values(X_b_z)\n",
    "shap.summary_plot(shap_values, X_b_z)\n",
    "shap.plots.heatmap(e(X_b_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b64b8-ca00-46d1-ba16-91f3adb00065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4871c2-4418-491a-8e5c-5776d42e4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f\n",
    "\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "residuals_normalized = lm.get_influence().resid_studentized_internal\n",
    "cooks = lm.get_influence().cooks_distance[0]\n",
    "cooks = np.round(f.pdf(cooks,len(lm.tvalues)+1, len(lm.fittedvalues)-len(lm.tvalues)-1),2)\n",
    "\n",
    "res_std = lm.get_influence().resid_std\n",
    "\n",
    "leverage = lm.get_influence().hat_matrix_diag\n",
    "\n",
    "plt.hist(pd.DataFrame(leverage))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pd.DataFrame(cooks))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pd.DataFrame(residuals_normalized))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "testNormal(residuals_normalized)\n",
    "\n",
    "w = res_std\n",
    "x = cooks\n",
    "y = leverage\n",
    "z = residuals_normalized\n",
    "\n",
    "fitted_y = lm.fittedvalues\n",
    "\n",
    "labels_ = fitted_y.index\n",
    "\n",
    "outlier_check = pd.concat([pd.DataFrame(x),pd.DataFrame(y),pd.DataFrame(z),pd.DataFrame(w)],axis=1).set_index(labels_)\n",
    "\n",
    "outlier_check.columns =  ['cooks', 'leverage', 'tsres', 'sres']\n",
    "\n",
    "qq = ProbPlot(residuals_normalized)\n",
    "\n",
    "c_thresh = .1\n",
    "l_thresh = (2*(len(lm.tvalues)-1)/len(lm.fittedvalues))\n",
    "s_thresh = max(qq.theoretical_quantiles)\n",
    "\n",
    "print(\"Outlier threshold's\")\n",
    "print(\"Cooks distance: > .1+\")\n",
    "print(\"Leverage: > \" + str(l_thresh) + \" to \" + str(3 * (len(lm.tvalues)-1)/len(fitted_y)))\n",
    "print(\"Studentized residuals: > \" + str(s_thresh))\n",
    "print()\n",
    "\n",
    "flag = []\n",
    "\n",
    "for i in range(0,len(outlier_check)):\n",
    "    if( (outlier_check.iloc[i][0] >= c_thresh) or (outlier_check.iloc[i][1] >= l_thresh) or (abs(outlier_check.iloc[i][2]) >= s_thresh) ):\n",
    "        print(outlier_check.iloc[i])\n",
    "        print()\n",
    "        flag.append(True)\n",
    "    else:\n",
    "        flag.append(False)\n",
    "\n",
    "outlier_check = pd.concat([outlier_check,pd.DataFrame(flag).set_index(labels_)],axis=1)\n",
    "\n",
    "outlier_check.columns =  ['cooks', 'leverage', 'tsres', 'sres', 'flagged']\n",
    "\n",
    "print(np.flip(np.argsort(cooks), 0))\n",
    "#print(outlier_check)\n",
    "\n",
    "search = outlier_check[outlier_check['flagged']==1].index.to_list()\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in search:\n",
    "    v = outlier_check.index.to_list().index(i)\n",
    "    rows.append(v)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2b196-7761-4420-a145-a5411053a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_style(row):\n",
    "\n",
    "    color = 'white'\n",
    "    if bool(row.flagged) == True:\n",
    "        color = 'orange'\n",
    "\n",
    "    return ['background-color: %s' % color]*len(row.values)\n",
    "\n",
    "all_data[set(all_data.columns) & set(New_Names)]\n",
    "\n",
    "outlier_check.style.apply(custom_style, axis=1).apply(custom_style, axis=1).background_gradient(cmap ='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da39b86-0ac2-4470-9457-8e999918d47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b8430-ac88-4358-88c1-e6c5400fe7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = y_train.columns.to_list()\n",
    "df2 = list(set(set(all_data.columns) & set(New_Names)))\n",
    "\n",
    "flattened = [] \n",
    "for sublist in df1,df2: \n",
    "    for val in sublist: \n",
    "        flattened.append(val) \n",
    "\n",
    "all_data.iloc[rows][flattened].style.background_gradient(cmap ='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

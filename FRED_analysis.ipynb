{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3139d-1ac0-4486-80f0-e54d08259ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee726465-0b88-46f7-877f-5aee84616ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import shap\n",
    "import numpy as np\n",
    "import ZCA as zca\n",
    "import statsmodels.api as sm\n",
    "import matplotlib as plt\n",
    "\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython\n",
    "\n",
    "import os\n",
    "os.environ['R_HOME'] = '/mnt/distvol/R/4.0.5/lib64/R/'\n",
    "import rpy2.robjects as R\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import arange\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import seaborn as sns\n",
    "from ModelDiagnostics import Plot\n",
    "from sklearn.cluster import DBSCAN\n",
    "from clustergram import Clustergram\n",
    "import urbangrammar_graphics as ugg\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from scipy.stats import f\n",
    "\n",
    "import dtale\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf8827-4f34-413b-9a4d-bd6565e33372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#power = PowerTransformer(method='box-cox')\n",
    "\n",
    "def testNormal (x):    \n",
    "    \n",
    "    k2, p = stats.normaltest(x)\n",
    "    alpha = .001\n",
    "    #print(\"p = {:g}\".format(p))    \n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        #print(p)\n",
    "        #print(alpha)\n",
    "        print(\"The null hypothesis can be rejected\")\n",
    "        xt, _ = stats.yeojohnson(x)\n",
    "        #xt, _ = stats.boxcox(x)        \n",
    "        print(_)\n",
    "        xt = pd.DataFrame(xt)\n",
    "        \n",
    "        return _, pd.DataFrame(xt).set_index(x.index)\n",
    "    else:\n",
    "        print(\"The null hypothesis cannot be rejected\")    \n",
    "        return 1, pd.DataFrame(x)\n",
    "\n",
    "def inverse_boxcox (data, lambdas):\n",
    "    return inv_boxcox(data, lambdas.values)\n",
    "    \n",
    "def transform_boxcox_l(data, l_):\n",
    "    transformed = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        #print(i)\n",
    "        if l_.iloc[i].values == 1:\n",
    "            inner_scale = data.iloc[:,i]            \n",
    "        else:\n",
    "            inner_scale = pd.DataFrame(stats.boxcox((data.iloc[:,i]), lmbda=l_.iloc[i].values))\n",
    "            \n",
    "        inner_scale.index = data.index\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed\n",
    "\n",
    "def transform_boxcox (data):\n",
    "    transformed = pd.DataFrame()\n",
    "    transformed_lambdas = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        l, inner_scale = testNormal(data.iloc[:,i])\n",
    "        inner_scale.set_index(data.index)\n",
    "\n",
    "        transformed_lambdas = pd.concat([transformed_lambdas,pd.DataFrame(pd.Series(l))],axis=0)\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed, transformed_lambdas\n",
    "\n",
    "def revert_boxcox (data, lambdas):\n",
    "    reverted = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):        \n",
    "        if lambdas.iloc[i].values == 1 :\n",
    "            revert = data.iloc[:,i]\n",
    "        else:\n",
    "            revert = pd.DataFrame(inv_boxcox(data.iloc[:,i].values, lambdas.iloc[i].values))            \n",
    "        revert.index = data.index\n",
    "        reverted = pd.concat([reverted,revert],axis=1)\n",
    "        \n",
    "    reverted.columns = data.columns\n",
    "    return reverted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9d431-186e-4171-8a8a-378ab2ab2e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_deltas(data):\n",
    "\n",
    "    R.r('''\n",
    "               f <- function(values) {\n",
    "                        #system(\"which openssl\")\n",
    "\n",
    "                        library(snpEnrichment)\n",
    "                        library(arfima)\n",
    "                        library(parallel)\n",
    "                        library(forecast)                    \n",
    "\n",
    "                        dset <- lapply(1:ncol(values),function(x)\n",
    "                        {\n",
    "                            column = values[,x]\n",
    "\n",
    "\n",
    "                            #tryCatch(invisible(capture.output(suppressMessages(suppressWarnings(\n",
    "                            #{\n",
    "                              varvefd = arfima(column)\n",
    "                              d = summary(varvefd)$coef[[1]][1]\n",
    "                              return(d)\n",
    "                            #}\n",
    "                           #)\n",
    "                           #))),\n",
    "                            #error=function(e)\n",
    "                              #{\n",
    "                                #d = 1\n",
    "                                #return(d)\n",
    "                              #})\n",
    "\n",
    "                        })    \n",
    "\n",
    "                        unlist(dset)\n",
    "\n",
    "                }\n",
    "                ''')\n",
    "\n",
    "    r_f = R.globalenv['f']\n",
    "    d=R.conversion.rpy2py((r_f(R.conversion.py2rpy(data.dropna()))))\n",
    "    return(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a133da3-b0fd-4741-a0aa-db8cc75f878d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_weights(d, num_k):\n",
    "    r\"\"\"Calculate weights ($w$) for each lag ($k$) through\n",
    "    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$.\n",
    "    \n",
    "    Args:\n",
    "        d (int): differencing value.\n",
    "        num_k (int): number of lags (typically length of timeseries) to calculate w.\n",
    "    \"\"\"\n",
    "    w_k = np.array([1])\n",
    "    \n",
    "    for k in range(1, num_k):\n",
    "        w_k = np.append(w_k, -w_k[-1] * ((d - k + 1)) / k)\n",
    "        \n",
    "    w_k = w_k.reshape(-1, 1) \n",
    "    \n",
    "    return w_k\n",
    "\n",
    "def get_weights_floored(d, num_k, floor=1e-3):\n",
    "    r\"\"\"Calculate weights ($w$) for each lag ($k$) through\n",
    "    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$ provided weight above a minimum value\n",
    "    (floor) for the weights to prevent computation of weights for the entire\n",
    "    time series.\n",
    "    \n",
    "    Args:\n",
    "        d (int): differencing value.\n",
    "        num_k (int): number of lags (typically length of timeseries) to calculate w.\n",
    "        floor (float): minimum value for the weights for computational efficiency.\n",
    "    \"\"\"\n",
    "    w_k = np.array([1])\n",
    "    k = 1\n",
    "    \n",
    "    while k < num_k:\n",
    "        w_k_latest = -w_k[-1] * ((d - k + 1)) / k\n",
    "        if abs(w_k_latest) <= floor:\n",
    "            break\n",
    "\n",
    "        w_k = np.append(w_k, w_k_latest)\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    w_k = w_k.reshape(-1, 1) \n",
    "    \n",
    "    return w_k\n",
    "\n",
    "def frac_diff(df, d, floor=1e-3):\n",
    "    r\"\"\"Fractionally difference time series via CPU.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe of raw time series values.\n",
    "        d (float): differencing value from 0 to 1 where > 1 has no FD.\n",
    "        floor (float): minimum value of weights, ignoring anything smaller.\n",
    "    \"\"\"\n",
    "    # Get weights window\n",
    "    weights = get_weights_floored(d=d, num_k=len(df), floor=floor)\n",
    "    weights_window_size = len(weights)\n",
    "    \n",
    "    # Reverse weights\n",
    "    weights = weights[::-1]\n",
    "    \n",
    "    # Blank fractionally differenced series to be filled\n",
    "    df_fd = []\n",
    "\n",
    "    # Slide window of time series, to calculated fractionally differenced values\n",
    "    # per window\n",
    "    for idx in range(weights_window_size, df.shape[0]):\n",
    "        # Dot product of weights and original values\n",
    "        # to get fractionally differenced values\n",
    "        date_idx = df.index[idx]\n",
    "        df_fd.append(np.dot(weights.T, df.iloc[idx - weights_window_size:idx]).item())\n",
    "    \n",
    "    # Return FD values and weights\n",
    "    df_fd = pd.DataFrame(df_fd)\n",
    "    \n",
    "    return df_fd, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e53d3-2864-4d69-a45a-ce48217e4fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949ef06-ae96-4828-b02b-21a92a11b29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6233b-8abf-4f78-83f5-a3e7a2e26fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data = pd.read_csv('/mnt/distvol/combined_set.csv')\n",
    "all_data.index = all_data.iloc[:,0]\n",
    "all_data = all_data.iloc[:,1:]\n",
    "\n",
    "filter_ = all_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b9636-4857-4aaa-b13d-e2077e957c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(Y):\n",
    "    \n",
    "    #Y = x\n",
    "    #output_slider_variable.value\n",
    "    internalFilter = filter_.copy()\n",
    "    internalFilter.remove(Y)\n",
    "    all_data_ = pd.concat([all_data[Y],all_data[internalFilter]], axis=1)    \n",
    "    #print(all_data_.describe())\n",
    "    display(all_data_.describe())\n",
    "    x_ticks = all_data_.index[np.arange(0, len(all_data.index), 5)]\n",
    "    plt.plot(all_data_[Y])\n",
    "    plt.xticks(x_ticks, rotation = 45)\n",
    "    plt.show()        \n",
    "    plt.hist(all_data_[Y], bins='auto')\n",
    "    plt.show()\n",
    "    diff = pd.DataFrame((all_data_[Y]-all_data_[Y].shift(-1))).dropna()\n",
    "    plt.hist(diff, bins='auto')\n",
    "    plt.show()\n",
    "    return(all_data_)\n",
    "    \n",
    "out = interactive(f3, Y=filter_)\n",
    "\n",
    "#output_slider_variable.observe(f4, 'value')\n",
    "\n",
    "print(\"choose Y\")\n",
    "display(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1c5c4-9718-4cbb-a820-9fd94e3fc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_deltas(out.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492f94d-cba1-455a-b1d4-157a346f6d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = dtale.show(out.result)\n",
    "d_.open_browser()\n",
    "d_._url  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4273fdd8-d805-4928-b7dd-bda427cf51fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future at 0x7f2f36260040 state=finished returned ndarray>, <Future at 0x7f2f36260070 state=finished returned ndarray>, <Future at 0x7f2f362620d0 state=finished returned ndarray>, <Future at 0x7f2f36262100 state=finished returned ndarray>, <Future at 0x7f2f36262130 state=finished returned ndarray>, <Future at 0x7f2f3625a160 state=finished returned ndarray>, <Future at 0x7f2f362a62e0 state=finished returned ndarray>, <Future at 0x7f2f36262310 state=finished returned ndarray>, <Future at 0x7f2f3625a340 state=finished returned ndarray>, <Future at 0x7f2f36262370 state=finished returned ndarray>, <Future at 0x7f2f36260490 state=finished returned ndarray>, <Future at 0x7f2f3625a490 state=finished returned ndarray>, <Future at 0x7f2f362604c0 state=finished returned ndarray>, <Future at 0x7f2f3625a4c0 state=finished returned ndarray>, <Future at 0x7f2f36260550 state=finished returned ndarray>, <Future at 0x7f2f362ac5e0 state=finished returned ndarray>, <Future at 0x7f2f362605e0 state=finished returned ndarray>, <Future at 0x7f2f362626a0 state=finished returned ndarray>, <Future at 0x7f2f36262730 state=finished returned ndarray>, <Future at 0x7f2f36262760 state=finished returned ndarray>, <Future at 0x7f2f362628b0 state=finished returned ndarray>, <Future at 0x7f2f362608e0 state=finished returned ndarray>, <Future at 0x7f2f366049d0 state=finished returned ndarray>, <Future at 0x7f2f3625a9d0 state=finished returned ndarray>, <Future at 0x7f2f3625aa00 state=finished returned ndarray>, <Future at 0x7f2f3625aa60 state=finished returned ndarray>, <Future at 0x7f2f362acac0 state=finished returned ndarray>, <Future at 0x7f2f3625aac0 state=finished returned ndarray>, <Future at 0x7f2f364a2b20 state=finished returned ndarray>, <Future at 0x7f2f3625ab20 state=finished returned ndarray>, <Future at 0x7f2f362acb50 state=finished returned ndarray>, <Future at 0x7f2f36260bb0 state=finished returned ndarray>, <Future at 0x7f2f362acbe0 state=finished returned ndarray>, <Future at 0x7f2f36262be0 state=finished returned ndarray>, <Future at 0x7f2f36260be0 state=finished returned ndarray>, <Future at 0x7f2f36262c10 state=finished returned ndarray>, <Future at 0x7f2f36260c10 state=finished returned ndarray>, <Future at 0x7f2f362a6c70 state=finished returned ndarray>, <Future at 0x7f2f362acca0 state=finished returned ndarray>, <Future at 0x7f2f36262cd0 state=finished returned ndarray>, <Future at 0x7f2f36260d00 state=finished returned ndarray>, <Future at 0x7f2f58182d30 state=finished returned ndarray>, <Future at 0x7f2f36262d30 state=finished returned ndarray>, <Future at 0x7f2f36262d90 state=finished returned ndarray>, <Future at 0x7f3031764df0 state=finished returned ndarray>, <Future at 0x7f2f3625ae20 state=finished returned ndarray>, <Future at 0x7f2f362a8e80 state=finished returned ndarray>, <Future at 0x7f2f3625ae80 state=finished returned ndarray>, <Future at 0x7f2f36262eb0 state=finished returned ndarray>, <Future at 0x7f2f3625aee0 state=finished returned ndarray>, <Future at 0x7f2f362a8fa0 state=finished returned ndarray>, <Future at 0x7f2f365a6fa0 state=finished returned ndarray>, <Future at 0x7f2f36257040 state=finished returned ndarray>, <Future at 0x7f2f4b5c10a0 state=finished returned ndarray>, <Future at 0x7f2f3625d0d0 state=finished returned ndarray>, <Future at 0x7f2f362bb130 state=finished returned ndarray>, <Future at 0x7f2f4b54f220 state=finished returned ndarray>, <Future at 0x7f2f362bb220 state=finished returned ndarray>, <Future at 0x7f2f3629f2b0 state=finished returned ndarray>, <Future at 0x7f2f362572e0 state=finished returned ndarray>, <Future at 0x7f2f3625d310 state=finished returned ndarray>, <Future at 0x7f2f36257370 state=finished returned ndarray>, <Future at 0x7f2f362a7430 state=finished returned ndarray>, <Future at 0x7f2f3625d4f0 state=finished returned ndarray>, <Future at 0x7f2f3625d5e0 state=finished returned ndarray>, <Future at 0x7f2f3625d610 state=finished returned ndarray>, <Future at 0x7f2f3629f700 state=finished returned ndarray>, <Future at 0x7f2f36257730 state=finished returned ndarray>, <Future at 0x7f2f36257820 state=finished returned ndarray>, <Future at 0x7f2f362bb850 state=finished returned ndarray>, <Future at 0x7f2f36257880 state=finished returned ndarray>, <Future at 0x7f2f3625d8b0 state=finished returned ndarray>, <Future at 0x7f2f363038e0 state=finished returned ndarray>, <Future at 0x7f2f36257970 state=finished returned ndarray>, <Future at 0x7f2f3625d9d0 state=finished returned ndarray>, <Future at 0x7f2f3625da30 state=finished returned ndarray>, <Future at 0x7f2f362bba60 state=finished returned ndarray>, <Future at 0x7f2f36257a60 state=finished returned ndarray>, <Future at 0x7f2f4b599b20 state=finished returned ndarray>, <Future at 0x7f2f362a7b20 state=finished returned ndarray>, <Future at 0x7f2f362b3b20 state=finished returned ndarray>, <Future at 0x7f2f3625dc10 state=finished returned ndarray>, <Future at 0x7f2f362bbc40 state=finished returned ndarray>, <Future at 0x7f2f36257d60 state=finished returned ndarray>, <Future at 0x7f2f3625de20 state=finished returned ndarray>, <Future at 0x7f2f362a7eb0 state=finished returned ndarray>, <Future at 0x7f2f3625deb0 state=finished returned ndarray>, <Future at 0x7f2f362a7ee0 state=finished returned ndarray>, <Future at 0x7f2f36257f10 state=finished returned ndarray>, <Future at 0x7f2f4b5b9f70 state=finished returned ndarray>, <Future at 0x7f2f3629ffd0 state=finished returned ndarray>}, not_done=set())"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from concurrent.futures import wait, ALL_COMPLETED\n",
    "from fracdiff import fdiff\n",
    "\n",
    "cores = int(len(os.sched_getaffinity(0)))\n",
    "\n",
    "def getDifferenced(i):\n",
    "    #v = d[[i]]\n",
    "    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i], d=d[[i]], floor=5e-5)\n",
    "    \n",
    "    a = np.array(out.result.iloc[:, i])\n",
    "    \n",
    "    return fdiff(a, n=d[i], axis=0)\n",
    "    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i]), d=v, floor=5e-5)\n",
    "\n",
    "pool01 = concurrent.futures.ProcessPoolExecutor(cores)\n",
    "\n",
    "futures01 = [pool01.submit(getDifferenced, args) for args in range(0,len(d))]\n",
    "\n",
    "wait(futures01, timeout=None, return_when=ALL_COMPLETED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "acbfa758-3cf3-4410-bfc6-85105b37a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Differenced_Set = pd.DataFrame()\n",
    "for f in range(0,len(futures01)):\n",
    "    value = pd.DataFrame(futures01[f].result())\n",
    "    Differenced_Set = pd.concat([Differenced_Set,value],axis=1)\n",
    "    \n",
    "Differenced_Set.columns = out.result.columns\n",
    "Differenced_Set.index = out.result.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e2820-110a-4786-9035-98543c301b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for f in range(0,len(futures01)):\n",
    "        #print(f)\n",
    "        #print(len(futures01[f].result()))\n",
    "        plt.hist(Differenced_Set.iloc[:,f], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "        Differenced_Set.iloc[:,1].plot()\n",
    "        plt.show()\n",
    "        plt.hist(all_data.iloc[:,f], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "        all_data.iloc[:,1].plot()\n",
    "        plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce862ad-f7fc-4f0b-b376-c29d32d01281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b6b7d-d0c5-4077-b822-d79e626067b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49f670-bbdb-42a8-b9e3-e75e1b60e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = out.result.corr()\n",
    "#.abs()\n",
    "s = c.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0016982-6857-424e-868a-da44b16ff66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece4be0-8244-434e-b66e-9c1f39a7c2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a92b7-e5b3-49ed-b31f-5959f90e8fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "def critical_r(n, alpha = .05 ):\n",
    "    df = n - 2\n",
    "    critical_t = scipy.stats.t.isf(alpha / 2, df)\n",
    "    critical_r = np.sqrt( (critical.t^2) / ( (critical.t^2) + df ) )\n",
    "    return(critical_r)\n",
    "\n",
    "def xcorr(x, y, maxlags=10):\n",
    "    Nx = len(x)\n",
    "    if Nx != len(y):\n",
    "        raise ValueError('x and y must be equal length')\n",
    "\n",
    "    c = np.correlate(x, y, mode=2)\n",
    "\n",
    "    if maxlags is None:\n",
    "        maxlags = Nx - 1\n",
    "\n",
    "    if maxlags >= Nx or maxlags < 1:\n",
    "        raise ValueError('maxlags must be None or strictly positive < %d' % Nx)\n",
    "\n",
    "    c = c[Nx - 1 - maxlags:Nx + maxlags]\n",
    "\n",
    "    return c\n",
    "\n",
    "def getLagged_Set(set_, og):\n",
    "\n",
    "    #set_.index = all_data.index\n",
    "\n",
    "    maxl = 5\n",
    "    Lagged_Differenced_Set = pd.DataFrame()\n",
    "    Lagged_Set = pd.DataFrame()\n",
    "    #TrainO_Lagged_Set = pd.DataFrame()\n",
    "    lags = []\n",
    "    lagcorrs = []\n",
    "    ogcorrs = []\n",
    "\n",
    "    for f in range(1,len(set_.columns)):\n",
    "        #print(f)\n",
    "        #print(len(futures01[f].result()))\n",
    "\n",
    "        data_1 = set_.iloc[:,0]\n",
    "        data_2 = set_.iloc[:,f]\n",
    "\n",
    "        ogc = np.array(pd.concat([data_1 - np.mean(data_1),data_2 - np.mean(data_2)],axis=1).corr())[1,0]    \n",
    "        ogcorrs.append(ogc)\n",
    "\n",
    "        #corr = xcorr(data_1 - np.mean(data_1), data_2 - np.mean(data_2),maxlags=5)\n",
    "\n",
    "        set1 = data_1 - np.mean(data_1)\n",
    "        set2 = data_2 - np.mean(data_2)\n",
    "\n",
    "        corrs_ = []\n",
    "        for i in range(0,maxl):\n",
    "            c = np.array((pd.concat([set1,set2.shift(i)],axis=1).dropna()).corr())[0,1]\n",
    "            corrs_.append(c)\n",
    "\n",
    "        #corr = np.correlate(data_1 - np.mean(data_1), data_2 - np.mean(data_2),mode='full')\n",
    "        #plt.plot(corr)\n",
    "        #plt.show()\n",
    "\n",
    "        #lag = corr.argmax() - (len(data_1) - 1)\n",
    "        lag = abs(pd.Series(corrs_)).idxmax()\n",
    "\n",
    "        #print(corr)\n",
    "        lagc = np.array(pd.concat([data_1 - np.mean(data_1),(data_2 - np.mean(data_2)).shift(lag)],axis=1).corr())[1,0]\n",
    "\n",
    "        #print(lag)\n",
    "\n",
    "        #print(ogc)\n",
    "        #print(lagc)\n",
    "\n",
    "        #print(lag)\n",
    "        #plt.plot(data_1, 'r*')\n",
    "        #plt.plot(data_2, 'b*')\n",
    "\n",
    "        lag_merged = pd.concat([data_1 - np.mean(data_1),(data_2 - np.mean(data_2)).shift(lag)],axis=1)\n",
    "\n",
    "        x_ticks = all_data.index[np.arange(0, len(all_data.index), 5)]\n",
    "        #plt.xticks(x_ticks, rotation = 45)    \n",
    "        #plt.show()\n",
    "\n",
    "        #plt.scatter(data_2.shift(lag),data_1)\n",
    "        #plt.show()\n",
    "\n",
    "        #plot_acf(data_2.shift(lag))\n",
    "        #plt.show()\n",
    "\n",
    "        #plot_pacf(data_2.shift(lag))\n",
    "        #plt.show()\n",
    "\n",
    "        y = data_1\n",
    "        X = data_2\n",
    "        #reg = LinearRegression().fit(X, y)\n",
    "\n",
    "        #print(reg.score(X, y),reg.coef_,reg.intercept_)\n",
    "\n",
    "        model = sm.OLS(y,X)\n",
    "        results = model.fit()\n",
    "        #print(results.summary())\n",
    "\n",
    "        Lagged_Differenced_Set = pd.concat([Lagged_Differenced_Set,data_2.shift(lag)],axis=1)\n",
    "        #TrainO_Lagged_Set = pd.concat([TrainO_Lagged_Set,all_data.iloc[:,f].shift(lag)],axis=1)\n",
    "        #if lag>0:\n",
    "            #lag = 0\n",
    "\n",
    "        Lagged_Set = pd.concat([Lagged_Set,og.iloc[:,f].shift(lag)],axis=1)\n",
    "\n",
    "        lagcorrs.append(lagc)\n",
    "\n",
    "        lags.append(lag)\n",
    "\n",
    "    stats = pd.concat([pd.DataFrame(set_.columns[1:]),pd.DataFrame(lags),pd.DataFrame(lagcorrs),pd.DataFrame(ogcorrs)],axis=1)\n",
    "        \n",
    "    return stats, pd.concat([data_1,Lagged_Set],axis=1),  pd.concat([data_1,Lagged_Differenced_Set],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3e80173c-962f-4b0e-b170-8186d5f9b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lagged_Set = pd.DataFrame()\n",
    "Lagged_Differenced_Set = pd.DataFrame()\n",
    "\n",
    "ls_stats, Lagged_Set, Lagged_Differenced_Set = getLagged_Set(Differenced_Set, out.result)\n",
    "lso_stats, Lagged_Set_offset, Lagged_Differenced_Set_offset = getLagged_Set(pd.concat([Differenced_Set.iloc[:,0].shift(-1),Differenced_Set.iloc[:,1:]],axis=1), out.result)\n",
    "                \n",
    "Lagged_Set_offset.dropna(inplace= True)\n",
    "\n",
    "Lagged_Differenced_Set.dropna(inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "29c83e18-ed0b-446d-89a3-86ea42e5672c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^SP500TR</th>\n",
       "      <th>DGS10</th>\n",
       "      <th>DTB3</th>\n",
       "      <th>DGS3MO</th>\n",
       "      <th>MORTGAGE30US</th>\n",
       "      <th>DFII10</th>\n",
       "      <th>T5YIFR</th>\n",
       "      <th>BAMLHYH0A0HYM2TRIV</th>\n",
       "      <th>BAMLCC0A1AAATRIV</th>\n",
       "      <th>DGS1</th>\n",
       "      <th>...</th>\n",
       "      <th>MRK</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NKE</th>\n",
       "      <th>PG</th>\n",
       "      <th>TRV</th>\n",
       "      <th>UNH</th>\n",
       "      <th>VZ</th>\n",
       "      <th>WMT</th>\n",
       "      <th>WBA</th>\n",
       "      <th>DIS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-09-30</th>\n",
       "      <td>-0.012870</td>\n",
       "      <td>2.842221</td>\n",
       "      <td>0.928281</td>\n",
       "      <td>0.947812</td>\n",
       "      <td>6.033846</td>\n",
       "      <td>1.534489</td>\n",
       "      <td>2.458380</td>\n",
       "      <td>142.546125</td>\n",
       "      <td>111.839104</td>\n",
       "      <td>1.219219</td>\n",
       "      <td>...</td>\n",
       "      <td>19.960261</td>\n",
       "      <td>5.431928</td>\n",
       "      <td>2.413771</td>\n",
       "      <td>33.315460</td>\n",
       "      <td>22.987969</td>\n",
       "      <td>8.841159</td>\n",
       "      <td>5.102193</td>\n",
       "      <td>9.245523</td>\n",
       "      <td>22.388096</td>\n",
       "      <td>16.798414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-31</th>\n",
       "      <td>0.057999</td>\n",
       "      <td>2.246544</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.761198</td>\n",
       "      <td>4.595468</td>\n",
       "      <td>1.309160</td>\n",
       "      <td>2.633988</td>\n",
       "      <td>144.586512</td>\n",
       "      <td>104.613166</td>\n",
       "      <td>1.121658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.584362</td>\n",
       "      <td>5.641008</td>\n",
       "      <td>3.090144</td>\n",
       "      <td>34.060504</td>\n",
       "      <td>13.396768</td>\n",
       "      <td>13.162466</td>\n",
       "      <td>5.340272</td>\n",
       "      <td>9.835456</td>\n",
       "      <td>20.486946</td>\n",
       "      <td>9.643745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-31</th>\n",
       "      <td>0.029813</td>\n",
       "      <td>2.034091</td>\n",
       "      <td>0.744966</td>\n",
       "      <td>0.691051</td>\n",
       "      <td>3.782129</td>\n",
       "      <td>1.333929</td>\n",
       "      <td>2.481008</td>\n",
       "      <td>133.264506</td>\n",
       "      <td>94.586836</td>\n",
       "      <td>0.960719</td>\n",
       "      <td>...</td>\n",
       "      <td>12.908632</td>\n",
       "      <td>5.890978</td>\n",
       "      <td>2.567263</td>\n",
       "      <td>33.880795</td>\n",
       "      <td>12.736108</td>\n",
       "      <td>14.034077</td>\n",
       "      <td>2.654544</td>\n",
       "      <td>8.180100</td>\n",
       "      <td>19.592208</td>\n",
       "      <td>9.305043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-06-30</th>\n",
       "      <td>-0.003927</td>\n",
       "      <td>2.104262</td>\n",
       "      <td>0.875694</td>\n",
       "      <td>0.811685</td>\n",
       "      <td>4.064521</td>\n",
       "      <td>1.281342</td>\n",
       "      <td>2.410222</td>\n",
       "      <td>108.856999</td>\n",
       "      <td>91.316710</td>\n",
       "      <td>1.474427</td>\n",
       "      <td>...</td>\n",
       "      <td>14.255809</td>\n",
       "      <td>4.451522</td>\n",
       "      <td>1.779560</td>\n",
       "      <td>33.825614</td>\n",
       "      <td>9.849975</td>\n",
       "      <td>14.126242</td>\n",
       "      <td>2.554620</td>\n",
       "      <td>5.024975</td>\n",
       "      <td>20.717374</td>\n",
       "      <td>6.169089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-09-30</th>\n",
       "      <td>0.040415</td>\n",
       "      <td>1.858598</td>\n",
       "      <td>1.240973</td>\n",
       "      <td>1.170057</td>\n",
       "      <td>3.561250</td>\n",
       "      <td>1.417925</td>\n",
       "      <td>2.453002</td>\n",
       "      <td>120.985879</td>\n",
       "      <td>86.109158</td>\n",
       "      <td>1.663095</td>\n",
       "      <td>...</td>\n",
       "      <td>12.550405</td>\n",
       "      <td>3.970997</td>\n",
       "      <td>1.843155</td>\n",
       "      <td>34.562184</td>\n",
       "      <td>4.958140</td>\n",
       "      <td>13.760587</td>\n",
       "      <td>2.209837</td>\n",
       "      <td>5.531028</td>\n",
       "      <td>20.872367</td>\n",
       "      <td>4.487760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>-0.004005</td>\n",
       "      <td>0.531741</td>\n",
       "      <td>1.806536</td>\n",
       "      <td>1.636014</td>\n",
       "      <td>2.181687</td>\n",
       "      <td>-0.158119</td>\n",
       "      <td>1.834268</td>\n",
       "      <td>258.672106</td>\n",
       "      <td>178.962768</td>\n",
       "      <td>1.779425</td>\n",
       "      <td>...</td>\n",
       "      <td>62.981894</td>\n",
       "      <td>44.894879</td>\n",
       "      <td>21.005691</td>\n",
       "      <td>117.781530</td>\n",
       "      <td>27.166114</td>\n",
       "      <td>68.514844</td>\n",
       "      <td>9.975453</td>\n",
       "      <td>25.499680</td>\n",
       "      <td>30.112728</td>\n",
       "      <td>22.190481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>-0.035486</td>\n",
       "      <td>0.178624</td>\n",
       "      <td>1.666975</td>\n",
       "      <td>1.476999</td>\n",
       "      <td>1.869560</td>\n",
       "      <td>-0.540136</td>\n",
       "      <td>1.755550</td>\n",
       "      <td>201.409451</td>\n",
       "      <td>200.035514</td>\n",
       "      <td>1.468621</td>\n",
       "      <td>...</td>\n",
       "      <td>60.052770</td>\n",
       "      <td>54.874905</td>\n",
       "      <td>19.346246</td>\n",
       "      <td>116.199859</td>\n",
       "      <td>41.974289</td>\n",
       "      <td>70.999950</td>\n",
       "      <td>10.150117</td>\n",
       "      <td>33.787033</td>\n",
       "      <td>32.788487</td>\n",
       "      <td>42.485933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>0.137470</td>\n",
       "      <td>-0.338343</td>\n",
       "      <td>1.319551</td>\n",
       "      <td>1.115835</td>\n",
       "      <td>1.608807</td>\n",
       "      <td>-0.940988</td>\n",
       "      <td>1.572655</td>\n",
       "      <td>327.442576</td>\n",
       "      <td>204.092265</td>\n",
       "      <td>1.067946</td>\n",
       "      <td>...</td>\n",
       "      <td>63.748302</td>\n",
       "      <td>60.629326</td>\n",
       "      <td>33.974816</td>\n",
       "      <td>113.627895</td>\n",
       "      <td>39.002418</td>\n",
       "      <td>83.513280</td>\n",
       "      <td>12.643137</td>\n",
       "      <td>39.352044</td>\n",
       "      <td>37.769869</td>\n",
       "      <td>38.373307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>0.075139</td>\n",
       "      <td>-0.114075</td>\n",
       "      <td>0.933277</td>\n",
       "      <td>0.730669</td>\n",
       "      <td>1.760076</td>\n",
       "      <td>-0.843875</td>\n",
       "      <td>1.482939</td>\n",
       "      <td>337.781474</td>\n",
       "      <td>178.639279</td>\n",
       "      <td>0.854431</td>\n",
       "      <td>...</td>\n",
       "      <td>62.007905</td>\n",
       "      <td>76.978450</td>\n",
       "      <td>51.262426</td>\n",
       "      <td>130.212312</td>\n",
       "      <td>23.160201</td>\n",
       "      <td>98.721896</td>\n",
       "      <td>13.225959</td>\n",
       "      <td>45.231862</td>\n",
       "      <td>31.704027</td>\n",
       "      <td>33.913499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-31</th>\n",
       "      <td>0.091881</td>\n",
       "      <td>0.229702</td>\n",
       "      <td>0.491868</td>\n",
       "      <td>0.307194</td>\n",
       "      <td>1.618877</td>\n",
       "      <td>-0.735417</td>\n",
       "      <td>1.715246</td>\n",
       "      <td>356.290356</td>\n",
       "      <td>147.010749</td>\n",
       "      <td>0.404244</td>\n",
       "      <td>...</td>\n",
       "      <td>59.726365</td>\n",
       "      <td>64.086581</td>\n",
       "      <td>43.420599</td>\n",
       "      <td>137.843919</td>\n",
       "      <td>16.131053</td>\n",
       "      <td>92.244560</td>\n",
       "      <td>9.654288</td>\n",
       "      <td>30.787923</td>\n",
       "      <td>25.087817</td>\n",
       "      <td>18.391832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ^SP500TR     DGS10      DTB3    DGS3MO  MORTGAGE30US    DFII10  \\\n",
       "Unnamed: 0                                                                   \n",
       "2004-09-30 -0.012870  2.842221  0.928281  0.947812      6.033846  1.534489   \n",
       "2004-12-31  0.057999  2.246544  0.795238  0.761198      4.595468  1.309160   \n",
       "2005-03-31  0.029813  2.034091  0.744966  0.691051      3.782129  1.333929   \n",
       "2005-06-30 -0.003927  2.104262  0.875694  0.811685      4.064521  1.281342   \n",
       "2005-09-30  0.040415  1.858598  1.240973  1.170057      3.561250  1.417925   \n",
       "...              ...       ...       ...       ...           ...       ...   \n",
       "2020-03-31 -0.004005  0.531741  1.806536  1.636014      2.181687 -0.158119   \n",
       "2020-06-30 -0.035486  0.178624  1.666975  1.476999      1.869560 -0.540136   \n",
       "2020-09-30  0.137470 -0.338343  1.319551  1.115835      1.608807 -0.940988   \n",
       "2020-12-31  0.075139 -0.114075  0.933277  0.730669      1.760076 -0.843875   \n",
       "2021-03-31  0.091881  0.229702  0.491868  0.307194      1.618877 -0.735417   \n",
       "\n",
       "              T5YIFR  BAMLHYH0A0HYM2TRIV  BAMLCC0A1AAATRIV      DGS1  ...  \\\n",
       "Unnamed: 0                                                            ...   \n",
       "2004-09-30  2.458380          142.546125        111.839104  1.219219  ...   \n",
       "2004-12-31  2.633988          144.586512        104.613166  1.121658  ...   \n",
       "2005-03-31  2.481008          133.264506         94.586836  0.960719  ...   \n",
       "2005-06-30  2.410222          108.856999         91.316710  1.474427  ...   \n",
       "2005-09-30  2.453002          120.985879         86.109158  1.663095  ...   \n",
       "...              ...                 ...               ...       ...  ...   \n",
       "2020-03-31  1.834268          258.672106        178.962768  1.779425  ...   \n",
       "2020-06-30  1.755550          201.409451        200.035514  1.468621  ...   \n",
       "2020-09-30  1.572655          327.442576        204.092265  1.067946  ...   \n",
       "2020-12-31  1.482939          337.781474        178.639279  0.854431  ...   \n",
       "2021-03-31  1.715246          356.290356        147.010749  0.404244  ...   \n",
       "\n",
       "                  MRK       MSFT        NKE          PG        TRV        UNH  \\\n",
       "Unnamed: 0                                                                      \n",
       "2004-09-30  19.960261   5.431928   2.413771   33.315460  22.987969   8.841159   \n",
       "2004-12-31  11.584362   5.641008   3.090144   34.060504  13.396768  13.162466   \n",
       "2005-03-31  12.908632   5.890978   2.567263   33.880795  12.736108  14.034077   \n",
       "2005-06-30  14.255809   4.451522   1.779560   33.825614   9.849975  14.126242   \n",
       "2005-09-30  12.550405   3.970997   1.843155   34.562184   4.958140  13.760587   \n",
       "...               ...        ...        ...         ...        ...        ...   \n",
       "2020-03-31  62.981894  44.894879  21.005691  117.781530  27.166114  68.514844   \n",
       "2020-06-30  60.052770  54.874905  19.346246  116.199859  41.974289  70.999950   \n",
       "2020-09-30  63.748302  60.629326  33.974816  113.627895  39.002418  83.513280   \n",
       "2020-12-31  62.007905  76.978450  51.262426  130.212312  23.160201  98.721896   \n",
       "2021-03-31  59.726365  64.086581  43.420599  137.843919  16.131053  92.244560   \n",
       "\n",
       "                   VZ        WMT        WBA        DIS  \n",
       "Unnamed: 0                                              \n",
       "2004-09-30   5.102193   9.245523  22.388096  16.798414  \n",
       "2004-12-31   5.340272   9.835456  20.486946   9.643745  \n",
       "2005-03-31   2.654544   8.180100  19.592208   9.305043  \n",
       "2005-06-30   2.554620   5.024975  20.717374   6.169089  \n",
       "2005-09-30   2.209837   5.531028  20.872367   4.487760  \n",
       "...               ...        ...        ...        ...  \n",
       "2020-03-31   9.975453  25.499680  30.112728  22.190481  \n",
       "2020-06-30  10.150117  33.787033  32.788487  42.485933  \n",
       "2020-09-30  12.643137  39.352044  37.769869  38.373307  \n",
       "2020-12-31  13.225959  45.231862  31.704027  33.913499  \n",
       "2021-03-31   9.654288  30.787923  25.087817  18.391832  \n",
       "\n",
       "[67 rows x 91 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp = pd.concat([out.result.iloc[:,0].pct_change(),Lagged_Differenced_Set_offset.iloc[:,1:]],axis=1).copy().dropna()\n",
    "Lagged_Differenced_Set_offset = pd.DataFrame()\n",
    "Lagged_Differenced_Set_offset = temp.copy()\n",
    "Lagged_Differenced_Set.dropna(inplace= True)\n",
    "Lagged_Differenced_Set_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfbc252-b567-48e9-86f9-9e57df0ac45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ls_stats)\n",
    "print(lso_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa28e7-3712-466f-96fd-7bbf115714ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lagged_Differenced_Set\n",
    "#from fracdiff import FracdiffStat\n",
    "#fs = FracdiffStat(window=100, mode=\"valid\")\n",
    "#diff = fs.fit_transform(nky.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb40926-6979-4f9f-ad09-ed1f93c25a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed, lambdas = transform_boxcox(Lagged_Differenced_Set.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1422b2e-d8e4-43aa-a691-8c54392a6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference (only if not using differenced)\n",
    "#transformed = (transformed - transformed.shift(-1)).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72722f-fbff-4bff-92b8-26b19888e415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc90237-f851-4afb-86f8-2dca0b41123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbd = True\n",
    "ic = True\n",
    "\n",
    "# evaluate an elastic net model on the dataset\n",
    "tsize = .20\n",
    "train, test = train_test_split(transformed.iloc[:,0:], test_size=tsize, shuffle=False)\n",
    "\n",
    "interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=ic)\n",
    "\n",
    "#train_t, lambdas_t = transform_boxcox(train)\n",
    "\n",
    "#disabled boxcox\n",
    "if cbd:\n",
    "    train_t = train\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_t)\n",
    "\n",
    "#\n",
    "train_s = pd.DataFrame(scaler.transform(train_t))\n",
    "train_s.columns = train.columns\n",
    "train_s.index = train.index  \n",
    "\n",
    "train_t = train_s\n",
    "\n",
    "#test_t = transform_boxcox_l(test, lambdas_t)\n",
    "\n",
    "#disabled boxcox\n",
    "if cbd:\n",
    "    test_t = test\n",
    "\n",
    "test_s = pd.DataFrame(scaler.transform(test_t))\n",
    "test_s.columns = test.columns\n",
    "test_s.index = test.index\n",
    "\n",
    "test_t = test_s\n",
    "\n",
    "y_train = pd.DataFrame(train_t.iloc[:,0])\n",
    "\n",
    "#exclude y\n",
    "\n",
    "X_inter_train = pd.DataFrame(interaction.fit_transform(train_t.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(train_t.iloc[:,1:]).columns))\n",
    "\n",
    "    #apply ZCA each time a set of factors are removed (i.e. iteratively)\n",
    " #trf = zca.ZCA().fit(X_inter_train)\n",
    "  #trf = zca.ZCA().fit(X_train)\n",
    "\n",
    " #X_train = pd.DataFrame(trf.transform(X_inter_train))\n",
    "  #X_train = pd.DataFrame(trf.transform(X_train))\n",
    " #X_train.columns=X_inter_train.columns\n",
    "  #X_train.columns=X_train.columns\n",
    "  #X_train.index = train.index\n",
    "\n",
    "#X_inter_alt = X_train.iloc[:, np.array(range(0,len(all_data.iloc[:,2:].columns)))]\n",
    "#print(X_inter_alt.head(3))\n",
    "\n",
    "y_test = pd.DataFrame(test_t.iloc[:,0])\n",
    "\n",
    "X_inter_test = pd.DataFrame(interaction.fit_transform(test_t.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(test_t.iloc[:,1:]).columns))\n",
    "\n",
    " #X_test = pd.DataFrame(trf.transform(X_inter_test))\n",
    "  #X_test = pd.DataFrame(trf.transform(X_test))\n",
    "\n",
    " #X_test.columns=X_inter_test.columns\n",
    "  #X_test.columns=X_test.columns\n",
    "  #X_test.index = test.index\n",
    "\n",
    "#X_inter_t_alt = X_test.iloc[:, np.array(range(0,len(all_data.iloc[:,2:].columns)))]\n",
    "#X_inter_t_alt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf46d0-1162-46d6-a166-0461b321e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ratios = arange(0, 1, 0.01)\n",
    "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "\n",
    " #model = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=4, verbose=0, precompute='auto')\n",
    "\n",
    "model = ElasticNet()\n",
    "grid = dict()\n",
    "# fit model\n",
    "\n",
    "grid['alpha'] = alphas\n",
    "grid['l1_ratio'] = ratios\n",
    "\n",
    "#search = HalvingRandomSearchCV(model, grid,resource='n_samples',max_resources=10,random_state=0)\n",
    "\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    " #results = search.fit(X_inter_train, y_train)\n",
    "#results = search.fit(X_train, y_train)\n",
    "\n",
    "results = search.fit(X_inter_train, y_train)\n",
    "\n",
    " #model.fit(X_inter_train, y_train)\n",
    "# summarize chosen configuration\n",
    "\n",
    "print('MAE: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)\n",
    "\n",
    "print(results.best_estimator_)\n",
    "best_model = ElasticNet(alpha=results.best_estimator_.alpha, l1_ratio = results.best_estimator_.l1_ratio)\n",
    "\n",
    "#pd.concat([all_data[Y],all_data_int],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb7965-c50b-4f1e-bd5e-05504a69192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_inter_train,train_t.iloc[:,0])\n",
    "\n",
    "trainScore = best_model.score(X_inter_train, y_train, sample_weight=None)\n",
    "testScore = best_model.score(X_inter_test, y_test, sample_weight=None)\n",
    "print(trainScore)\n",
    "print(testScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db6ef6-0c1f-4ff8-b8b2-3344feffa9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b12fd-2b69-4243-909a-6f828aa43b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86999a-16ba-4505-b5ec-843d219ac62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "14400790-a56a-4084-8c56-cf73697546e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 accuracy with a standard deviation of 0.08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8518518518518519"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#pd.DataFrame(Lagged_Set.iloc[:,0].shift(-1)).hist()\n",
    "#Lagged_Differenced_Set = (all_data - all_data.shift(-1)).dropna()\n",
    "#Lagged_Differenced_Set_f = pd.concat([Lagged_Differenced_Set.iloc[:,0].shift(-1),Lagged_Differenced_Set.iloc[:,1:]],axis=1)\n",
    "\n",
    "set_ = pd.DataFrame()\n",
    "set_ = Lagged_Differenced_Set_offset.copy()\n",
    "\n",
    "X, y = set_.iloc[:,1:],  set_.iloc[:,0]\n",
    "\n",
    "y = pd.DataFrame(np.where(y > 0, 1, 0))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "#clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_transformed = pd.DataFrame(scaler.transform(X_train))\n",
    "X_train_transformed.columns = X_train.columns\n",
    "\n",
    "X_inter_train = X_train_transformed\n",
    "\n",
    "#X_inter_train = pd.DataFrame(interaction.fit_transform(X_train_transformed), columns=interaction.get_feature_names(input_features=pd.DataFrame(X_train_transformed).columns))\n",
    "\n",
    "#trf = zca.ZCA().fit(X_inter_train)\n",
    "\n",
    "#X_inter_train = pd.DataFrame(trf.transform(X_inter_train))\n",
    "#X_inter_train.columns=pd.DataFrame(X_inter_train).columns\n",
    "#X_inter_train.index = X_train.index\n",
    "\n",
    "clf = svm.SVC(kernel='sigmoid', C=1, random_state=42)\n",
    "\n",
    "scores = cross_val_score(clf, X_inter_train, y_train, cv=5)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "clf = svm.SVC(C=1).fit(X_inter_train, y_train)\n",
    "\n",
    "X_test_transformed = pd.DataFrame(scaler.transform(X_test))\n",
    "X_test_transformed.columns = X_test.columns\n",
    "X_test_transformed.index = X_test.index\n",
    "\n",
    "X_inter_test = X_test_transformed\n",
    "#X_inter_test = pd.DataFrame(interaction.fit_transform(X_test_transformed), columns=interaction.get_feature_names(input_features=pd.DataFrame(X_test_transformed).columns))\n",
    "\n",
    "#X_inter_test = pd.DataFrame(trf.transform(X_inter_test))\n",
    "\n",
    "predicted = clf.predict(X_inter_test)\n",
    "#predicted.index = y_test.index\n",
    "\n",
    "clf.score(X_inter_test, y_test)\n",
    "#clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560215f-9b7d-4d76-ac61-dbf96d2cfd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4dfc37-ab80-427f-bc37-edb9a212c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate output multi-step 1d cnn example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dense, SimpleRNN, GRU, LSTM\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Lagged_Differenced_Set = (all_data - all_data.shift(-1)).dropna()\n",
    "\n",
    "def plot_learning_curves(loss, val_loss):\n",
    "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.\", label=\"Training loss\")\n",
    "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r-\", label=\"Validation loss\")\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out, xcolumns, ycolumns):\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, xcolumns], sequences[end_ix:out_end_ix, ycolumns]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return array(X), array(y)\n",
    "\n",
    "#n_steps_in = int(np.floor(len(train)*.8))\n",
    "print(len(Lagged_Differenced_Set))\n",
    "n_steps_in = int(np.round(len(Lagged_Differenced_Set)/2))\n",
    "print(len(Lagged_Differenced_Set) - n_steps_in)\n",
    "print(n_steps_in)\n",
    "#n_steps_out = int(len(train)-n_steps_in)\n",
    "n_steps_out = 1\n",
    "print(n_steps_out)\n",
    "\n",
    "#ycolumns = range(0,len(transformed.columns))\n",
    "ycolumns = range(0,len(Lagged_Differenced_Set.columns[0:1].values))\n",
    "xcolumns = range(1,len(Lagged_Differenced_Set.columns[1:]))\n",
    "\n",
    "#trainInner, valid = train_test_split(trainOuter, test_size=0.15, shuffle=False)\n",
    "#trainInner_whitened = pd.DataFrame(trf.transform(trainInner.iloc[:,1:])).set_index(trainInner.index)\n",
    "#valid_whitened = pd.DataFrame(trf.transform(valid.iloc[:,1:])).set_index(valid.index)\n",
    "#test_whitened = pd.DataFrame(trf.transform(test.iloc[:,1:])).set_index(test.index)\n",
    "\n",
    "trainOuter_whitened=pd.DataFrame(trf.transform(trainOuter.iloc[:,1:])).set_index(trainOuter.index)\n",
    "\n",
    "\n",
    "\n",
    "X, y = split_sequences(np.array(Lagged_Differenced_Set), n_steps_in, n_steps_out, xcolumns, ycolumns)\n",
    "\n",
    "#for i in range(len(X)):\n",
    "#    print(X[i], y[i])\n",
    "    \n",
    "#flatten output\n",
    "\n",
    "if len(ycolumns) > 0:\n",
    "    n_output = y.shape[1] * y.shape[2]\n",
    "    y = y.reshape((y.shape[0], n_output))\n",
    "\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model\n",
    "modelCNN = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(n_output)\n",
    "])\n",
    "\n",
    "# fit model\n",
    "#model.fit(X, y, epochs=7000, verbose=0)\n",
    "\n",
    "epochs_ = 200\n",
    "batch_size_ = 25\n",
    "\n",
    "#np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam', loss='mse')\n",
    "modelCNN.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "\n",
    "#model6.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    shuffle = False)\n",
    "                                                    #random_state=42)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_transformed = pd.DataFrame(scaler.transform(X_train))\n",
    "X_train_transformed.columns = X_train.columns\n",
    "\n",
    "X_test_transformed = pd.DataFrame(scaler.transform(X_test))\n",
    "X_test_transformed.columns = X_test.columns\n",
    "X_test_transformed.index = X_test.index\n",
    "\n",
    "    #new_series = pd.DataFrame(ts_train_scaled).append(pd.DataFrame(ts_valid_scaled)).append(pd.DataFrame(ts_test_scaled))\n",
    "\n",
    "    #series_reshaped = np.array([new_series[i:i + (n_steps+n_ahead)].copy() for i in range(len(data) - (n_steps+n_ahead))])  \n",
    "    \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,\n",
    "                                                    test_size=0.33,\n",
    "                                                    shuffle = False)\n",
    "                                                    #random_state=42)    \n",
    "\n",
    "#model6.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "historyCNN = modelCNN.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,validation_data=(X_valid, y_valid), verbose=0)\n",
    "#history6 = model6.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,validation_data=(X_valid, y_valid), verbose=0)\n",
    "#history = model.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f7e2d-3603-4ba1-8457-4fd9918498a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "plot_learning_curves(historyCNN.history[\"loss\"], history.history[\"val_loss\"])\n",
    "plt.show()\n",
    "#plot_learning_curves(history6.history[\"loss\"], history.history[\"val_loss\"])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f6d94-9757-4baa-b095-8937fbba5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4c952-450d-43d4-86c1-2a37673a58d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc3da-fa1f-4e57-9d84-f4e5f8872af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#multi\n",
    "yhat = modelCNN.predict(X_test, verbose=0)\n",
    "\n",
    "predicted = []\n",
    "original = []\n",
    "if len(ycolumns) > 1:\n",
    "    results = yhat.reshape(len(y_test), int(yhat.shape[1]/X_test.shape[2]), len(transformed.columns))\n",
    "    for i in range(0,len(results)):\n",
    "        predicted.append(pd.DataFrame(results[i])[0][0])\n",
    "        original.append(pd.DataFrame(y_test[i])[0][0])\n",
    "        #print(pd.DataFrame(results[i])[0][0])\n",
    "        #print(pd.DataFrame(y_test[i])[0][0])\n",
    "else:\n",
    "    results = yhat\n",
    "    for i in range(0,len(results)):\n",
    "        predicted.append(results[i])\n",
    "        original.append(y_test[i])\n",
    "        #print(pd.DataFrame(results[i]))\n",
    "        #print(pd.DataFrame(y_test[i]))\n",
    "        \n",
    "plt.scatter(predicted,original)\n",
    "\n",
    "pd.concat([pd.DataFrame(predicted),pd.DataFrame(original)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8229628-eed4-4b76-831d-1de033240347",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(best_model.coef_).set_index(X_inter_train.columns)\n",
    "\n",
    "a_coef = abs(coef)\n",
    "a_coef.sort_values(by=[0],ascending=False,inplace=True)\n",
    "chosen_few = a_coef[a_coef>0].dropna().index.values\n",
    "\n",
    "scaler_ = preprocessing.StandardScaler().fit(transformed)\n",
    "\n",
    "#\n",
    "train_ = pd.DataFrame(scaler_.transform(transformed))\n",
    "train_.columns = transformed.columns\n",
    "train_.index = transformed.index  \n",
    "\n",
    "X_inter_train_ = pd.DataFrame(interaction.fit_transform(train_.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(train_.iloc[:,1:]).columns))\n",
    "\n",
    "max_pvalue = 1\n",
    "New_Names = X_inter_train.columns\n",
    "X_b = X_inter_train_[chosen_few]\n",
    "while (max_pvalue > .05):\n",
    "        \n",
    "    trf = zca.ZCA().fit(X_b)\n",
    "        \n",
    "    X_b_z = pd.DataFrame(trf.transform(X_b))\n",
    "    X_b_z.columns=pd.DataFrame(X_b).columns\n",
    "    X_b_z.index = train_.index\n",
    "\n",
    "    model_ = sm.OLS(train_.iloc[:,0],sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip'))        \n",
    "    #model_ = sm.OLS(pd.DataFrame(pd.concat([train_.iloc[:,0],X_b_z],axis=1),sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip'))        \n",
    "    results_ = model_.fit()\n",
    "\n",
    "    set_ = X_b.columns.tolist()\n",
    "    \n",
    "    max_pvalue = max(results_.pvalues[1:])\n",
    "    if (max_pvalue > .05):\n",
    "        print(max_pvalue)\n",
    "        max_pname = (results_.pvalues[1:]).idxmax(axis=1)\n",
    "        set_.remove(max_pname)\n",
    "        New_Names = set_\n",
    "    \n",
    "        X_b = X_inter_train_[New_Names]\n",
    "        X_b.index = X_inter_train_.index\n",
    "\n",
    "#from statsmodels.formula.api import ols\n",
    "#lm = ols(pd.DataFrame(train_.iloc[:,0]) ~ sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip')).fit()\n",
    "#table = sm.stats.anova_lm(model_, type=3)\n",
    "#print(table)\n",
    "print(results_.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e428c-cce9-46e3-b145-754fc9e4c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNaNCols(df_):\n",
    "    for col in df_:        \n",
    "        num_NaNs = df_[col].isnull().sum()\n",
    "        if num_NaNs > 0:\n",
    "            print(f\"Column: {col}\")\n",
    "            print(f\"Number of NaNs: {num_NaNs}\")\n",
    "\n",
    "findNaNCols(X_b_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4e220-4ac2-4945-9e3a-4b8ff1355a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d940860-86d6-4768-81a0-63f9127143a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_plot = Plot.LinearRegressionResidualPlot(X_b_z, pd.DataFrame(train_.iloc[:,0]))\n",
    "lm = linear_plot.fit()\n",
    "summary, diag_res = linear_plot.diagnostic_plots(lm)\n",
    "print(\"Summary of Regression\\n:{}\".format(summary))\n",
    "print(\"Diagnostic Tests of Regression\\n:{}\".format(diag_res))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#sns.pairplot(pd.concat([pd.DataFrame(train[Y]),X_b_z],axis=1), hue=Y, height=2);\n",
    "\n",
    "pd.concat([pd.DataFrame(train[Y]),X_b_z],axis=1).hist()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model_s = sklearn.linear_model.LinearRegression()\n",
    "model_s.fit(X_b_z, pd.DataFrame(train_[Y]))\n",
    "\n",
    "shap.initjs()\n",
    "e = shap.explainers.Linear(model_s, X_b_z)\n",
    "\n",
    "shap_values = e.shap_values(X_b_z)\n",
    "shap.summary_plot(shap_values, X_b_z)\n",
    "shap.plots.heatmap(e(X_b_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b64b8-ca00-46d1-ba16-91f3adb00065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4871c2-4418-491a-8e5c-5776d42e4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f\n",
    "\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "residuals_normalized = lm.get_influence().resid_studentized_internal\n",
    "cooks = lm.get_influence().cooks_distance[0]\n",
    "cooks = np.round(f.pdf(cooks,len(lm.tvalues)+1, len(lm.fittedvalues)-len(lm.tvalues)-1),2)\n",
    "\n",
    "res_std = lm.get_influence().resid_std\n",
    "\n",
    "leverage = lm.get_influence().hat_matrix_diag\n",
    "\n",
    "plt.hist(pd.DataFrame(leverage))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pd.DataFrame(cooks))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pd.DataFrame(residuals_normalized))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "testNormal(residuals_normalized)\n",
    "\n",
    "w = res_std\n",
    "x = cooks\n",
    "y = leverage\n",
    "z = residuals_normalized\n",
    "\n",
    "fitted_y = lm.fittedvalues\n",
    "\n",
    "labels_ = fitted_y.index\n",
    "\n",
    "outlier_check = pd.concat([pd.DataFrame(x),pd.DataFrame(y),pd.DataFrame(z),pd.DataFrame(w)],axis=1).set_index(labels_)\n",
    "\n",
    "outlier_check.columns =  ['cooks', 'leverage', 'tsres', 'sres']\n",
    "\n",
    "qq = ProbPlot(residuals_normalized)\n",
    "\n",
    "c_thresh = .1\n",
    "l_thresh = (2*(len(lm.tvalues)-1)/len(lm.fittedvalues))\n",
    "s_thresh = max(qq.theoretical_quantiles)\n",
    "\n",
    "print(\"Outlier threshold's\")\n",
    "print(\"Cooks distance: > .1+\")\n",
    "print(\"Leverage: > \" + str(l_thresh) + \" to \" + str(3 * (len(lm.tvalues)-1)/len(fitted_y)))\n",
    "print(\"Studentized residuals: > \" + str(s_thresh))\n",
    "print()\n",
    "\n",
    "flag = []\n",
    "\n",
    "for i in range(0,len(outlier_check)):\n",
    "    if( (outlier_check.iloc[i][0] >= c_thresh) or (outlier_check.iloc[i][1] >= l_thresh) or (abs(outlier_check.iloc[i][2]) >= s_thresh) ):\n",
    "        print(outlier_check.iloc[i])\n",
    "        print()\n",
    "        flag.append(True)\n",
    "    else:\n",
    "        flag.append(False)\n",
    "\n",
    "outlier_check = pd.concat([outlier_check,pd.DataFrame(flag).set_index(labels_)],axis=1)\n",
    "\n",
    "outlier_check.columns =  ['cooks', 'leverage', 'tsres', 'sres', 'flagged']\n",
    "\n",
    "print(np.flip(np.argsort(cooks), 0))\n",
    "#print(outlier_check)\n",
    "\n",
    "search = outlier_check[outlier_check['flagged']==1].index.to_list()\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in search:\n",
    "    v = outlier_check.index.to_list().index(i)\n",
    "    rows.append(v)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2b196-7761-4420-a145-a5411053a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_style(row):\n",
    "\n",
    "    color = 'white'\n",
    "    if bool(row.flagged) == True:\n",
    "        color = 'orange'\n",
    "\n",
    "    return ['background-color: %s' % color]*len(row.values)\n",
    "\n",
    "all_data[set(all_data.columns) & set(New_Names)]\n",
    "\n",
    "outlier_check.style.apply(custom_style, axis=1).apply(custom_style, axis=1).background_gradient(cmap ='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da39b86-0ac2-4470-9457-8e999918d47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b8430-ac88-4358-88c1-e6c5400fe7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = y_train.columns.to_list()\n",
    "df2 = list(set(set(all_data.columns) & set(New_Names)))\n",
    "\n",
    "flattened = [] \n",
    "for sublist in df1,df2: \n",
    "    for val in sublist: \n",
    "        flattened.append(val) \n",
    "\n",
    "all_data.iloc[rows][flattened].style.background_gradient(cmap ='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
